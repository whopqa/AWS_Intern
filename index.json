[{"uri":"https://whopqa.github.io/AWS_Intern/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Improving accuracy for your cloud budgeting with new features in AWS Budgets by Fredrik Tunvall | on 29 APR 2025 | in Announcements, AWS Budgets, AWS Cloud Financial Management | Permalink | Share\nToday, AWS announced new capabilities in AWS Budgets that provides greater flexibility in how you track and manage your AWS spend. These enhancements include:\nSupport for additional cost metrics: net unblended costs and net amortized costs Ability to exclude specific dimension values when creating budgets (such as service, account, and instance type) New filtering capabilities for charge types for fine-grained control to include or exclude AWS Savings Plans (SPs) or Reservation (RI) upfront charges, recurring fees, taxes, and credits Enhanced API functionality that supports filter expressions that are consistent with AWS Cost Explorer These improvements enable you to create budgets that better reflect your actual costs, including discounts, and provide more granular control over what’s included in your budget calculations. They also let you create a budget with filter selections consistent with AWS Cost Explorer.\nEnhanced Capabilities for More Accurate Cost Tracking The new capabilities in AWS Budgets offer significant improvements in cost tracking accuracy and flexibility. By supporting net unblended and net amortized cost metrics, you can now create budgets that align precisely with your actual spend, including all applicable discounts. This enhancement is particularly valuable for companies using Savings Plans or Reserved Instances, as it allows for more accurate budget planning and tracking.\nThe introduction of advanced filtering capabilities, including the ability to exclude specific dimension values, opens up new possibilities for precise cost monitoring. These features enable several powerful use cases that were previously challenging to implement:\nGranular Cost Control: Creating targeted budgets is now much simpler. For example, you can easily exclude shared infrastructure costs when setting up budgets for specific research initiatives, providing a clearer view of project-specific expenses.\nCompliance and Governance: You can create region-specific budgets more effectively, aiding in compliance with data sovereignty requirements. You can now efficiently track costs for specific regions while excluding others, simplifying regulatory compliance and cost allocation.\nFinancial Planning: The support for new cost metrics allows you to create budgets that precisely match your cost reporting needs. You can now maintain more accurate forecasts that include all applicable discounts, leading to improved financial planning and decision-making.\nBy leveraging these new capabilities, you can gain deeper insights into your AWS spend, enabling more effective cost management and financial governance across your cloud environments.\nGetting Started in the AWS Console Prerequisite: To use these features, you need Cost Explorer IAM permissions (ce:GetCostAndUsage and ce:GetDimensionValues). Contact your account or IAM administrator to add these permissions. To learn more about access to billing and cost management tools, see Using identity-based policies (IAM policies) for AWS Cost Management.\nTo create a budget with the new capabilities:\nOpen the AWS Budgets console. Choose Create budget. Under Budget setup, choose Customize (advanced). For Budget metric, select your preferred option: Net unblended cost Net amortized cost Under Filters, configure your budget scope: Optionally exclude specific dimensions Choose charge types, e.g. Reservation Applied Usage and Savings Plan fees Set your budget amount and configure alert thresholds. API Changes and Backward Compatibility AWS Budgets is introducing two new fields, FilterExpression and Metrics, while maintaining support for the existing CostFilters and CostTypes fields. The new FilterExpression field provides the same filtering capabilities as Cost Explorer, allowing for more sophisticated filtering options including the ability to exclude specific dimension values. The Metrics field supports additional cost metrics such as net unblended and net amortized costs.\nIf you’re already using AWS Budgets, your existing budgets and API integrations will continue to function as before. When describing a budget, AWS Budgets will now include both the new FilterExpression and Metrics fields alongside the existing CostFilters and CostTypes fields in the response. This gives you a starting point for making further updates without having to build the entire Expression data type from scratch.\nWhen updating budgets, you can use either the old fields (CostFilters and CostTypes) or the new fields (FilterExpression and Metrics), but not both in the same API call. For configurations that can be expressed using both old and new fields, the API response will include both formats. However, when using new features like exclusions that cannot be represented in the old format, only the new fields will appear in the response.\nWorking with Budgets: CLI Examples Let’s walk through some examples to see how these capabilities work in practice.\nDescribing an existing budget\nAs a FinOps engineer, you need to audit your existing budget for your storage cost. To do so, you use the CLI commands below to describe your existing budget. This command will show your budget configuration with both old and new filters:\naws budgets describe-budget \\ --account-id 111122223333 \\ --budget-name \u0026#34;StorageCostsBudget\u0026#34; In the response, you’ll see both the old fields (CostFilters and CostTypes) and the new fields (FilterExpression and Metrics), allowing you to understand how your existing budget configuration translates to the new format.\n{ \u0026#34;Budget\u0026#34;: { \u0026#34;BudgetName\u0026#34;: \u0026#34;StorageCostsBudget\u0026#34;, \u0026#34;BudgetLimit\u0026#34;: { \u0026#34;Amount\u0026#34;: \u0026#34;1000.0\u0026#34;, \u0026#34;Unit\u0026#34;: \u0026#34;USD\u0026#34; }, // Previous filter representation \u0026#34;CostFilters\u0026#34;: { \u0026#34;Service\u0026#34;: [\u0026#34;Amazon Simple Storage Service\u0026#34;], \u0026#34;Region\u0026#34;: [\u0026#34;us-east-1\u0026#34;, \u0026#34;us-west-2\u0026#34;] }, \u0026#34;CostTypes\u0026#34;: { \u0026#34;IncludeCredit\u0026#34;: true, \u0026#34;IncludeDiscount\u0026#34;: true, \u0026#34;IncludeRefund\u0026#34;: false, \u0026#34;IncludeSubscription\u0026#34;: true, \u0026#34;UseBlended\u0026#34;: false }, // Equivalent new filter representation \u0026#34;FilterExpression\u0026#34;: { \u0026#34;Dimensions\u0026#34;: { \u0026#34;Key\u0026#34;: \u0026#34;SERVICE\u0026#34;, \u0026#34;Values\u0026#34;: [\u0026#34;Amazon Simple Storage Service\u0026#34;] }, \u0026#34;And\u0026#34;: [{ \u0026#34;Dimensions\u0026#34;: { \u0026#34;Key\u0026#34;: \u0026#34;REGION\u0026#34;, \u0026#34;Values\u0026#34;: [\u0026#34;us-east-1\u0026#34;, \u0026#34;us-west-2\u0026#34;] } }] }, \u0026#34;Metrics\u0026#34;: [\u0026#34;UNBLENDED_COST\u0026#34;], \u0026#34;TimeUnit\u0026#34;: \u0026#34;MONTHLY\u0026#34; } } Updating an existing budget\nNow that you’ve reviewed your storage budget, you want to refine it to include a new storage service, Amazon FSx. Here’s how you can update an existing budget:\naws budgets update-budget \\ --account-id 111122223333 \\ --cli-input-json file://update-budget.json # update-budget.json contents: { \u0026#34;NewBudget\u0026#34;: { \u0026#34;BudgetName\u0026#34;: \u0026#34;StorageCostsBudget\u0026#34;, \u0026#34;BudgetLimit\u0026#34;: { \u0026#34;Amount\u0026#34;: \u0026#34;1000.0\u0026#34;, \u0026#34;Unit\u0026#34;: \u0026#34;USD\u0026#34; }, \u0026#34;FilterExpression\u0026#34;: { \u0026#34;Dimensions\u0026#34;: { \u0026#34;Key\u0026#34;: \u0026#34;SERVICE\u0026#34;, \u0026#34;Values\u0026#34;: [\u0026#34;Amazon Simple Storage Service\u0026#34;, \u0026#34;Amazon FSx\u0026#34;] // add FSx }, \u0026#34;And\u0026#34;: [{ \u0026#34;Dimensions\u0026#34;: { \u0026#34;Key\u0026#34;: \u0026#34;REGION\u0026#34;, \u0026#34;Values\u0026#34;: [\u0026#34;us-east-1\u0026#34;, \u0026#34;us-west-2\u0026#34;] } }] }, \u0026#34;Metrics\u0026#34;: [\u0026#34;NET_UNBLENDED_COST\u0026#34;], \u0026#34;TimeUnit\u0026#34;: \u0026#34;MONTHLY\u0026#34; } } Creating a New Budget with Exclusion\nYou’re now ready to create a new monthly budget that excludes shared enterprise services to better track team-specific costs. Here’s how you can create a budget using the new exclusion capability:\naws budgets create-budget \\ --account-id 111122223333 \\ --cli-input-json file://create-budget.json # create-budget.json contents: { \u0026#34;Budget\u0026#34;: { \u0026#34;BudgetName\u0026#34;: \u0026#34;TeamSpecificBudget\u0026#34;, \u0026#34;BudgetType\u0026#34;: \u0026#34;COST\u0026#34;, \u0026#34;BudgetLimit\u0026#34;: { \u0026#34;Amount\u0026#34;: \u0026#34;5000.0\u0026#34;, \u0026#34;Unit\u0026#34;: \u0026#34;USD\u0026#34; }, \u0026#34;FilterExpression\u0026#34;: { \u0026#34;Not\u0026#34;: { \u0026#34;Dimensions\u0026#34;: { \u0026#34;Key\u0026#34;: \u0026#34;SERVICE\u0026#34;, \u0026#34;Values\u0026#34;: [\u0026#34;AWS Shield\u0026#34;, \u0026#34;AWS Support (Enterprise)\u0026#34;] } } }, \u0026#34;Metrics\u0026#34;: [\u0026#34;NET_UNBLENDED_COST\u0026#34;], \u0026#34;TimeUnit\u0026#34;: \u0026#34;MONTHLY\u0026#34; }, \u0026#34;NotificationsWithSubscribers\u0026#34;: [ { \u0026#34;Notification\u0026#34;: { \u0026#34;ComparisonOperator\u0026#34;: \u0026#34;GREATER_THAN\u0026#34;, \u0026#34;NotificationType\u0026#34;: \u0026#34;ACTUAL\u0026#34;, \u0026#34;Threshold\u0026#34;: 80, \u0026#34;ThresholdType\u0026#34;: \u0026#34;PERCENTAGE\u0026#34; }, \u0026#34;Subscribers\u0026#34;: [ { \u0026#34;Address\u0026#34;: \u0026#34;team-budget-alerts@example.com\u0026#34;, \u0026#34;SubscriptionType\u0026#34;: \u0026#34;EMAIL\u0026#34; } ] } ] } This new budget excludes shared services, allowing you to focus on team-specific costs. Note that when you describe this budget, you’ll see only the new fields (FilterExpression and Metrics) in the response, as the exclusion capability cannot be represented using the old fields.\nBest Practices for Leveraging the New Capabilities To make the most of these new features, consider the following best practices:\nReview your existing budgets to determine if the new metrics and filtering options can provide more accurate tracking of your actual costs. Use the net amortized or net unblended cost metrics to create budgets that match how you report on AWS costs internally. Leverage the new exclusion capabilities to create more targeted budgets for specific teams, projects, or cost centers by excluding shared or irrelevant costs. Create separate budgets for different types of costs (e.g., upfront fees vs. usage-based costs) to get a more detailed view of your spend patterns. If you have existing automation using the AWS Budgets API, plan a gradual transition to the new fields. Test thoroughly to ensure your integrations work as expected with the enhanced capabilities. Frequently Asked Questions How can I most easily convert my budgets to use the new filter options?\nYou can view your existing budget configurations in the console or through the DescribeBudget API, which will display both old and new field formats. This helps you see how your current filters translate to the new format.\nWhat if I don\u0026rsquo;t want to use the new features?\nYou can continue to use AWS Budgets as you do today, and simply ignore the new fields across all APIs. Your existing budgets will continue to function as before.\nCan I have some budgets that use old filters and some budgets that use new?\nYes, you can. When you use the DescribeBudget API or view budgets in the console, budgets will be shown with both old and new fields if the configuration can be represented in both formats. However, for budgets using new features like exclusions, which cannot be represented in the old format, only the new fields (FilterExpression and Metrics) will be present in the response.\nWhat changes do I need to make in my code so I don’t break anything?\nExisting code using the old fields (CostFilters and CostTypes) will continue to work as before. If you want to use the new capabilities, you’ll need to update your code to use FilterExpression and Metrics instead. Remember that you cannot mix old and new fields in the same API call.\nConclusion With these new capabilities, AWS Budgets offers you more flexibility and accuracy in managing your cloud spend. Whether you’re looking to track costs after discounts, create focused budgets for specific teams, or get a more granular view of your AWS expenses, these enhancements provide the tools you need for effective cloud financial management.\n"},{"uri":"https://whopqa.github.io/AWS_Intern/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Increase engagement with localized content using Amazon Bedrock Flows by Benjamin Le and Emilio Garcia Montano | on 01 MAY 2025 | in Amazon Bedrock, Amazon Bedrock Knowledge Bases, Amazon Bedrock Prompt Management, Amazon Machine Learning, Amazon Nova, Amazon Simple Storage Service (S3), Artificial Intelligence, Content Production, Data Science \u0026amp; Analytics for Media, Industries, Media \u0026amp; Entertainment, Storage | Permalink | Share\nContent creators and publishers often maintain large collections with thousands, if not hundreds of thousands, of articles that can be localized for new audiences and geographies to deliver higher engagement in emerging and newly targeted markets. Localization is generally the process of transforming content (vocabulary choice, tone adjustments, and translation) for a new audience from one geography to another.\nHuman-driven localization cannot scale to the necessary volume, so machine translation is commonly used. However, automated localization still struggles with quality—particularly in contextualizing content to capture the nuances of each specific market. This often results in lower engagement rates compared to the original content. With foundation models supporting a wide range of languages and dialects, media and entertainment customers are increasingly using generative AI to deliver higher-quality localized content.\nAmazon Bedrock Flows provides a no-code visual builder and a set of APIs to seamlessly connect leading state-of-the-art foundation models (such as Anthropic’s Claude and Amazon Nova) within Amazon Bedrock. It also integrates with other AWS services to automate user-defined generative AI workflows that go far beyond sending a prompt to a large language model.\nAmazon Bedrock Prompt Management provides a streamlined interface to create, evaluate, version, and share prompts. It helps developers and prompt engineers achieve the best responses from foundation models for their use cases.\nIn this post, we demonstrate how you can leverage Amazon Bedrock capabilities (such as Flows, Prompt Management, and various foundation models) to quickly build and test a workflow. This workflow takes existing content, produces localized copies, and provides an evaluation of the differences for editor review.\nScenario overview As an online publisher looking to expand readership across new geographies and channels—without relying solely on new local content—you want to create a workflow that:\nLocalizes existing text content for a specific country and language to better align with local markets and advertising strategies. Adapts content to new and emerging channels (such as short-form social media) using style guides currently stored in Amazon Bedrock Knowledge Bases to help editors review their work. Provides evaluation metrics (such as factual accuracy, length, dialect, and overall change in meaning) so editors can make informed decisions to publish or refine content further. Figure 1: High-level architecture diagram outlining a content management system using Amazon Bedrock Flows and Amazon Bedrock Knowledge Bases to localize content and provide evaluations for editor review.\nPrerequisites Before creating the flow and prompts, ensure you have the following setup:\nAn AWS account and an IAM user or role authorized to use Amazon Bedrock. Refer to: Getting started with Amazon Bedrock. Ensure the role includes permissions for Flows and Prompt Management as explained in the prerequisites for Amazon Bedrock Flows and Prompt Management. Access to the models you plan to invoke and evaluate. See: Manage access to Amazon Bedrock foundation models. Our example uses Amazon Nova Pro, Cohere Command R, Cohere Embed Multilingual v3, and Anthropic’s Claude 3.7 Sonnet in the Oregon Region (us-west-2). An Amazon Bedrock Knowledge Base configured with an Amazon S3 data source. In our example, the knowledge base contains style guides for creating social media content. Creating the prompts Before building the flow, create two prompts that will be used in the later prompt flow.\nThe first prompt performs content localization, and we use the Anthropic Claude 3.7 Sonnet foundation model, which supports languages such as English, French, Spanish, Portuguese, Japanese, and others.\nWhen designing prompts, variables such as the article text are represented by {{article}}, and the target language is represented by {{language}}, allowing values to be shared across flow nodes for greater flexibility.\nFigure 2: Content Localization prompt\nThe second prompt evaluates the original content and the localized content to provide an assessment for editors. To remain independent from the localization prompt, this evaluation prompt uses Amazon Nova Pro—a powerful multimodal model that provides the best combination of accuracy, speed, and cost across more than 200 languages.\nFigure 3: Content Evaluation prompt\nCreating and configuring the flow Using the Amazon Bedrock console, navigate to Flows under Builder Tools and create a new flow as shown in Figure 4.\nFigure 4: Amazon Bedrock Flow named \u0026ldquo;Content Localization\u0026rdquo;\nAfter creation, the console will automatically open the flow builder. Otherwise, select Edit in flow builder.\nInside the Flow Builder, you can choose from multiple node types. Drag nodes onto the canvas and link them after defining variables or loading saved prompts.\nIn this example, the flow is shown in Figure 5.\nFigure 5: Content localization flow\nDetails for each node:\nInput node simulates content coming from a CMS and accepts a JSON object with:\narticle: selected article text country: target country language: target language query: text prompt to query the knowledge base for relevant style guides Get_Style_Guides node uses Cohere Command R to interpret the query and retrieve results from the knowledge base. These results supplement localized content generation.\nLocalization Prompt node uses the earlier prompt to localize the article.\nEvaluation Prompt node evaluates the original and localized texts using the evaluation prompt.\nTwo Output nodes emit data produced by the respective prompt nodes.\nLLMs can generate hallucinations. Amazon Bedrock Flows integrates with Amazon Bedrock Guardrails so you can configure safeguards to detect or block undesired responses.\nTesting the flow Using the Test flow panel, you can quickly test the workflow with near real-time node outputs.\nTo illustrate how generative AI can adapt Spanish content across Europe and Latin America, the following examples use the same input text but change the target country.\nFigure 6: Example inputs\nFigure 7: Sample localized text\nFinally, the evaluation step uses a separate foundation model to independently compare original and localized content and produce a scored assessment.\nFigure 8: Example content evaluation\nPricing Amazon Bedrock Flows charges per 1,000 transitions within your workflow. A transition occurs whenever data moves from one node to another (for example, from an input node to a prompt node). There is no additional charge for using Amazon Bedrock Prompt Management.\nModel usage pricing depends on the model type and the number of input/output tokens. This example also uses Amazon Bedrock Knowledge Bases configured with an Amazon OpenSearch Serverless vector database and Amazon S3 as the data source.\nPricing varies by AWS Region. All resources in this walkthrough are configured in Oregon (us-west-2). Refer to pricing pages for Amazon Bedrock, Amazon S3, and Amazon OpenSearch Service.\nConclusion We have demonstrated how media and entertainment customers can build a localized content workflow using a serverless, low-code architecture. Using Amazon Bedrock Flows and Prompt Management, content owners and editors can leverage generative AI to deliver more engaging content to new audiences.\nContact an AWS Representative to learn how we can help accelerate your business.\nFurther reading Amazon Bedrock Flows User Guide:\nBuild an end-to-end generative AI workflow with Amazon Bedrock Flows\nAmazon Bedrock Prompt Management User Guide:\nConstruct and store reusable prompts with Prompt Management in Amazon Bedrock\nAmazon Bedrock Flows is now generally available with enhanced safety and traceability\nEvaluating prompts at scale with Prompt Management and Prompt Flows for Amazon Bedrock\n"},{"uri":"https://whopqa.github.io/AWS_Intern/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Innovating at Speed: BMW\u0026rsquo;s Generative AI Solution for Cloud Incident Analysis by Johann Wildgruber, Dr. Jens Kohl, Thilo Bindel, Luisa-Sophie Gloger, Aishwarya Lakshmi Krishnan, Huong Vu, Kim Robins, Otto Kruse, Satyam Saxena, and Tanrajbir Takher | March 05, 2025 | in Advanced (300), Amazon Bedrock, Amazon Bedrock Agents, Amazon CloudWatch, Artificial Intelligence, AWS CloudTrail, Customer Enablement, Customer Solutions | Permalink | Comments | Share\nThis post was co-authored with Johann Wildgruber, Dr. Jens Kohl, Thilo Bindel, and Luisa-Sophie Gloger from BMW Group.\nThe BMW Group — headquartered in Munich, Germany — employs over 154,000 people and operates 30 manufacturing and assembly plants worldwide, along with R\u0026amp;D centers in 17 countries. Today, BMW Group (BMW) is a global leader in premium automobiles and motorcycles, also offering high-end financial and mobility services.\nBMW Connected Company is a division within BMW responsible for developing and operating premium digital services for BMW’s connected fleet, which currently numbers more than 23 million vehicles worldwide. These digital services are used by many BMW vehicle owners daily; for example, to lock or open car doors remotely using an app on their phone, to start window defrost remotely, to buy navigation map updates from the car’s menu, or to listen to music streamed over the internet in their car.\nIn this post, we explain how BMW uses generative AI technology on AWS to help run these digital services with high availability. Specifically, BMW uses Amazon Bedrock Agents to make remediating (partial) service outages quicker by speeding up the otherwise cumbersome and time-consuming process of root cause analysis (RCA). The fully automated RCA agent correctly identifies the right root cause for most cases (measured at 85%), and helps engineers in terms of system understanding and real-time insights in their cases. This performance was further validated during the proof of concept, where employing the RCA agent on representative use cases clearly demonstrates the benefits of this solution, allowing BMW to achieve significantly lower diagnosis times.\nChallenges in Root Cause Analysis for Cloud Incidents Digital services are often implemented by chaining multiple software components together; components that might be built and run by different teams. For example, consider the service of remotely opening and locking vehicle doors. There might be a development team building and running the iOS app, another team for the Android app, a team building and running the backend-for-frontend used by both the iOS and Android app, and so on. Moreover, these teams might be geographically dispersed and run their workloads in different locations and regions; many hosted on AWS, some elsewhere.\nNow consider a (fictitious) scenario where reports come in from car owners complaining that remotely locking doors with the app no longer works. Is the iOS app responsible for the outage, or the backend-for-frontend? Did a firewall rule change somewhere? Did an internal TLS certificate expire? Is the MQTT system experiencing delays? Was there an inadvertent breaking change in recent API changes? When did they actually deploy that? Or was the database password for the central subscription service rotated again?\nIt can be difficult to determine the root cause of issues in situations like this. It requires checking many systems and teams, many of which might be failing, because they’re interdependent. Developers need to reason about the system architecture, form hypotheses, and follow the chain of components until they have located the one that is the culprit. They often have to backtrack and reassess their hypotheses, and pursue the investigation in another chain of components.\nUnderstanding the challenges in such complex systems highlights the need for a robust and efficient approach to root cause analysis. With this context in mind, let’s explore how BMW and AWS collaborated to develop a solution using Amazon Bedrock Agents to streamline and enhance the RCA process.\nSolution Overview At a high level, the solution uses an Amazon Bedrock agent to do automated RCA. This agent has several custom-built tools at its disposal to do its job. These tools, implemented by AWS Lambda functions, use services like Amazon CloudWatch and AWS CloudTrail to analyze system logs and metrics. The following diagram illustrates the solution architecture.\nWhen an incident occurs, an on-call engineer gives a description of the issue at hand to the Amazon Bedrock agent. The agent will then start investigating for the root cause of the issue, using its tools to do tasks that the on-call engineer would otherwise do manually, such as searching through logs. Based on the clues it uncovers, the agent proposes several likely hypotheses to the on-call engineer. The engineer can then resolve the issue, or give pointers to the agent to direct the investigation further. In the following section, we take a closer look at the tools the agent uses.\nAmazon Bedrock agent tools The Amazon Bedrock agent’s effectiveness in performing RCA lies in its ability to seamlessly integrate with custom tools. These tools, designed as Lambda functions, use AWS services like CloudWatch and CloudTrail to automate tasks that are typically manual and time-intensive for engineers. By organizing its capabilities into specialized tools, the Amazon Bedrock agent makes sure that RCA is both efficient and precise.\nArchitecture Tool The Architecture Tool uses C4 diagrams to provide a comprehensive view of the system’s architecture. These diagrams, enhanced through Structurizr, give the agent a hierarchical understanding of component relationships, dependencies, and workflows. This allows the agent to target the most relevant areas during its RCA process, effectively narrowing down potential causes of failure based on how different systems interact.\nFor instance, if an issue affects a specific service, the Architecture Tool can identify upstream or downstream dependencies and suggest hypotheses focused on those systems. This accelerates diagnostics by enabling the agent to reason contextually about the architecture instead of blindly searching through logs or metrics.\nLogs Tool The Logs Tool uses CloudWatch Logs Insights to analyze log data in real time. By searching for patterns, errors, or anomalies, as well as comparing the trend to the previous period, it helps the agent pinpoint issues related to specific events, such as failed authentications or system crashes.\nFor example, in a scenario involving database access failures, the Logs Tool might identify a new spike in the number of error messages such as “FATAL: password authentication failed” compared to the previous hour. This insight allows the agent to quickly associate the failure with potential root causes, such as an improperly rotated database password.\nMetrics Tool The Metrics Tool provides the agent with real-time insights into the system’s health by monitoring key metrics through CloudWatch. This tool identifies statistical anomalies in critical performance indicators such as latency, error rates, resource utilization, or unusual spikes in usage patterns, which can often signal potential issues or deviations from normal behavior.\nFor instance, in a Kubernetes memory overload scenario, the Metrics Tool might detect a sharp increase in memory consumption or unusual resource allocation prior to the failure. By surfacing CloudWatch metric alarms for such anomalies, the tool enables the agent to prioritize hypotheses related to resource mismanagement, misconfigured thresholds, or unexpected system load, guiding the investigation more effectively toward resolving the issue.\nInfrastructure Tool The Infrastructure Tool uses CloudTrail data to analyze critical control-plane events, such as configuration changes, security group updates, or API calls. This tool is particularly effective in identifying misconfigurations or breaking changes that might trigger cascading failures.\nConsider a case where a security group ingress rule is inadvertently removed, causing connectivity issues between services. The Infrastructure Tool can detect and correlate this event with the reported incident, providing the agent with actionable insights to guide its RCA process.\nBy combining these tools, the Amazon Bedrock agent mimics the step-by-step reasoning of an experienced engineer while executing tasks at machine speed. The modular nature of the tools allows for flexibility and customization, making sure that RCA is tailored to the unique needs of BMW’s complex, multi-regional cloud infrastructure.\nIn the next section, we discuss how these tools work together within the agent’s workflow.\nAmazon Bedrock Agents: ReAct Framework in Action At the heart of BMW’s rapid RCA lies the ReAct (Reasoning and Action) agent framework, an innovative approach that dynamically combines logical reasoning with task execution. By integrating ReAct with Amazon Bedrock, BMW gains a flexible solution for diagnosing and resolving complex cloud-based incidents. Unlike traditional methods, which rely on predefined workflows, ReAct agents use real-time inputs and iterative decision-making to adapt to the specific circumstances of an incident.\nThe ReAct agent in BMW’s RCA solution uses a structured yet adaptive workflow to diagnose and resolve issues. First, it interprets the textual description of an incident (for example, “Vehicle doors cannot be locked via the app”) to identify which parts of the system are most likely impacted. Guided by the ReAct framework’s iterative reasoning, the agent then gathers evidence by calling specialized tools, using data centrally aggregated in a cross-account observability setup. By continuously reevaluating the results of each tool invocation, the agent zeros in on potential causes—whether an expired certificate, a revoked firewall rule, or a spike in traffic—until it isolates the root cause. The following diagram illustrates this workflow.\nThe ReAct framework offers the following benefits:\nDynamic \u0026amp; adaptive: The ReAct agent tailors its approach to the specific incident, rather than a one-size-fits-all methodology. This adaptability is especially critical in BMW’s multi-regional, multi-service architecture. Efficient tool utilization: By reasoning about which tools to invoke and when, the ReAct agent minimizes redundant queries, providing faster diagnostics without overloading AWS services like CloudWatch or CloudTrail. Human-like reasoning: The ReAct agent mimics the logical thought process of a seasoned engineer, iteratively exploring hypotheses until it identifies the root cause. This capability bridges the gap between automation and human expertise. Case study: Root cause analysis “Unlocking vehicles via the iOS app” To illustrate the power of Amazon Bedrock agents in action, let us explore a possible real-world scenario involving the interplay between BMW’s connected fleet and the digital services running in the cloud backend.\nWe deliberately change the security group for the central networking account in a test environment. This has the effect that requests from the fleet are (correctly) blocked by the changed security group and do not reach the services hosted in the backend. Hence, a test user cannot lock or unlock her vehicle door remotely.\nIncident Details\nBMW engineers received a report from a tester indicating the remote lock/unlock functionality on the mobile app does not work.\nThis report raised immediate questions: was the issue in the app itself, the backend-for-frontend service, or deeper within the system, such as in the MQTT connectivity or authentication mechanisms?\nHow the ReAct agent addresses the problem\nThe agent begins by understanding the overall system architecture, calling the Architecture Tool. The outputs of the architecture tool reveal that the iOS app, like the Android app, is connected to a backend-for-frontend API, and that the backend-for-frontend API itself is connected to several other internal APIs, such as the Remote Vehicle Management API. The Remote Vehicle Management API is responsible for sending commands to cars by using MQTT messaging.\nThe agent uses the other tools at its disposal in a targeted way: it scans the logs, metrics, and control plane activities of only those components that are involved in remotely unlocking car doors: iOS app remote logs, backend-for-frontend API logs, and so on. The agent finds several clues:\na. Anomalous logs that indicate connectivity issues (network timeouts).\nb. A sharp decrease in the number of successful invocations of the Remote Vehicle Management API.\nc. Control plane activities: several security groups in the central networking account hosted on the testing environment were changed.\nBased on those findings, the agent infers and defines several hypotheses and presents these to the user, ordered by their likelihood. In this case, the first hypothesis is the actual root cause: a security group was inadvertently changed in the central networking account, which meant that network traffic between the backend-for-frontend and the Remote Vehicle Management API was now blocked. The agent correctly correlated logs (“fetch timeout error”), metrics (decrease in invocations) and control plane changes (security group ingress rule removed) to come to this conclusion.\nIf the on-call engineer wants further information, they can now ask follow-up questions to the agent, or instruct the agent to investigate elsewhere as well.\nThe entire process—from incident detection to resolution—took minutes, compared to the hours it could have taken with traditional RCA methods. The ReAct agent’s ability to dynamically reason, access cross-account observability data, and iterate on its hypotheses alleviated the need for tedious manual investigations.\nThe entire diagnosis took minutes instead of hours.\nConclusion By using Amazon Bedrock ReAct agents, BMW has shown how to improve its approach to root cause analysis, turning a complex and manual process into an efficient, automated workflow. The tools integrated within the ReAct framework significantly narrow down potential reasoning space, and enable dynamic hypotheses generation and targeted diagnostics, mimicking the reasoning process of seasoned engineers while operating at machine speed. This innovation has reduced the time required to identify and resolve service disruptions, further enhancing the reliability of BMW’s connected services and improving the experience for millions of customers worldwide.\nThe solution has demonstrated measurable success, with the agent identifying root causes in 85% of test cases and providing detailed insights in the remainder, greatly expediting engineers’ investigations. By lowering the barrier to entry for junior engineers, it has enabled less-experienced team members to diagnose issues effectively, maintaining reliability and scalability across BMW’s operations.\nIncorporating generative AI into RCA processes showcases the transformative potential of AI in modern cloud-based operations. The ability to adapt dynamically, reason contextually, and handle complex, multi-regional infrastructures makes Amazon Bedrock Agents a game changer for organizations aiming to maintain high availability in their digital services.\nAs BMW continues to expand its connected fleet and digital offerings, the adoption of generative AI-driven solutions like Amazon Bedrock will play an important role in maintaining operational excellence and delivering seamless experiences to customers. By following BMW’s example, your organization can also benefit from Amazon Bedrock Agents for root cause analysis to enhance service reliability.\nGet started by exploring Amazon Bedrock Agents to optimize your incident diagnostics or use CloudWatch Logs Insights to identify anomalies in your system logs. If you want a hands-on introduction to creating your own Amazon Bedrock agents—complete with code examples and best practices—check out the following GitHub repository. These tools are setting a new industry standard for efficient RCA and operational excellence.\n"},{"uri":"https://whopqa.github.io/AWS_Intern/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Phan Quốc Anh\nPhone Number: 0379729847\nEmail: pqa1085@gmail.com\nUniversity: FPT University\nMajor: Artificial Intelligent\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 09/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://whopqa.github.io/AWS_Intern/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Get familiar with members of the First Cloud Journey (FCJ) program and understand the rules and regulations at the internship unit. Understand basic concepts of cloud computing and the AWS service ecosystem (Compute, Storage, Networking, Database, Security…). Practice creating and managing an AWS Free Tier account following best practices: IAM User, Group, Role; avoid using the root account; set up Budgets for cost monitoring. Become familiar with the AWS Management Console and AWS CLI to interact with resources, along with introduction to scripting for automation. Understand cost optimization mechanisms: Spot Instances, AWS Pricing Calculator, Budget/Cost Budget. Get familiar with Serverless concepts and the Hugo tool for deploying workshops/static websites. Understand and practice basic AWS networking: VPC, Subnet, Route Table, Security Group, \u0026hellip; Understand basic networking concepts: CIDR, IP Address, Port, Protocol… Understand VPC Security, VPN, Direct Connect, Load Balancer… Tasks to carry out this week: Day Task Start Date Completion Date Reference Material 2 - Complete Module 1 theory content. - Learn to create workshops using Hugo: + Download and set up Hugo + Learn how to create weekly worklogs 8/9/2025 8/9/2025 https://www.notion.so/WEEK-1-269fd6be9818815f8e4bf7180ff107c8?source=copy_link 3 - Study foundational computer networking concepts: + CIDR + IP Address + Port Number + Protocol 9/9/2025 9/9/2025 Networking documents + AWS Academy 4 - Complete labs 1, 7, 9 of Module 1: AWS overview, AWS infrastructure, AWS policies, AWS Services, budget-based cost optimization. 10/9/2025 10/9/2025 https://www.notion.so/WEEK-1-269fd6be9818815f8e4bf7180ff107c8?source=copy_link 5 - Learn basic VPC concepts: + VPC + Security Group + NACL + VPC Flow Logs + VPC Peering + Transit Gateway + VPC Endpoint 11/9/2025 11/9/2025 https://www.notion.so/WEEK-1-269fd6be9818815f8e4bf7180ff107c8?source=copy_link 6 - Study on-premise to AWS connectivity: + Site-to-Site VPN + Client VPN + AWS Direct Connect - Learn Elastic Load Balancing and traffic distribution mechanisms 12/9/2025 12/9/2025 https://www.notion.so/WEEK-1-269fd6be9818815f8e4bf7180ff107c8?source=copy_link Week 1 Achievements: Completed Module 1 theory and clearly understood:\nCloud Computing models AWS service groups: Compute, Storage, Networking, Database… Became familiar with Hugo:\nSuccessfully downloaded and installed Hugo Created and managed weekly worklogs using Hugo Gained solid fundamental networking knowledge:\nCIDR, IP Address Port number, Protocol Network segmentation \u0026amp; how IP addressing works Completed AWS Module 1 labs:\nAWS Global Infrastructure AWS Services and security policies Cost optimization using AWS Budget \u0026amp; contacting support Understood basic VPC concepts:\nSecurity Group (instance-level security) NACL (subnet-level security) Flow Logs, Peering, Endpoint, Transit Gateway Understood on-premise to AWS connectivity:\nSite-to-Site VPN Client VPN Direct Connect Learned Elastic Load Balancing and how load distribution works.\nBuilt a solid foundational knowledge base to prepare for the next week focusing on EC2, CLI, and advanced networking.\n"},{"uri":"https://whopqa.github.io/AWS_Intern/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Deepen understanding of VPC architecture and its components: Subnet, IGW, NAT Gateway, Route Table, Security Group. Understand network monitoring and analysis through VPC Flow Logs \u0026amp; Reachability Analyzer. Deploy EC2 and practice access using Session Manager. Familiarize with and practice the VPC Site-to-Site model. Learn the concept of Infrastructure as Code and apply it to AWS infrastructure. Configure Hybrid DNS using Route 53 Resolver. Study RDGW – Remote Desktop Gateway in a Hybrid environment. Practice Transit Gateway and multi-VPC routing. Study Module 3 theory: EC2, EBS, AMI, Auto Scaling, metadata, pricing options, EFS, FSx, Lightsail, MGN. Complete labs on AWS Backup, Resource Tagging, and CloudWatch Monitoring. Deploy the FCJ Management application using Auto Scaling Group. Practice Lightsail for cost optimization. Tasks to carry out this week: Day Tasks Start Date Completion Date Reference Material Monday - Complete Lab 3:\n+ VPC, Subnet, Internet Gateway + Route Table, Security Group, NAT Gateway - Enable VPC Flow Logs - Deploy EC2 - Practice VPC Site-to-Site - Reachability Analyzer - Session Manager - Deploy CloudWatch Monitoring 2025-09-15 2025-09-15 https://docs.google.com/document/d/1JWeyNBbld7xkbBnHOszgusOyCnk4IecfjyDEp51FXJk/edit?usp=sharing Tuesday - Learn Infrastructure as Code - Complete Lab 10: Hybrid DNS with Route 53 Resolver - Study RDGW 2025-09-16 2025-09-16 Provided document link Wednesday - Complete Lab 20: Configure Transit Gateway 2025-09-17 2025-09-17 Provided document link Thursday - Study Module 3 theory: + AMI / Backup / Key Pair + Amazon EBS + EC2 / Instance Store + EC2 User Data \u0026amp; Metadata + EC2 Auto Scaling + Pricing Options + Amazon Lightsail + EFS + FSx + MGN - Complete Lab 13: AWS Backup - Lab 27: Manage resources with Tag \u0026amp; Resource Group 2025-09-18 2025-09-18 Provided document link Friday - Complete Lab 8: AWS CloudWatch Workshop - Lab 6: Deploy FCJ Management application using Auto Scaling Group - Lab 45: Lightsail Workshop – Cost Optimization 2025-09-19 2025-09-19 Provided document link Week 2 Achievements: Completed all labs on VPC and advanced Networking, including:\nCreating a fully configured VPC with IGW, NAT Gateway, and custom Route Tables Setting up proper Security Groups Enabling VPC Flow Logs Verifying connectivity using Reachability Analyzer Deployed EC2 and practiced modern access techniques:\nSSM Session Manager (no SSH key required) CloudWatch Monitoring for CPU, Network, and Log Insights Successfully built the VPC Site-to-Site model and understood cross-network routing.\nLearned and practiced Infrastructure as Code, combined with Route 53 Resolver to build Hybrid DNS.\nStudied RDGW and understood how to access resources in a Hybrid environment.\nCompleted Transit Gateway lab with multi-VPC routing.\nFinished all Module 3 theory topics:\nAMI, EBS, EC2, Auto Scaling Instance Store, Metadata/User Data Pricing Options (On-demand, Reserved, Spot) Lightsail, EFS, FSx, MGN Completed Lab 13 – AWS Backup, building a full data protection solution.\nUsed Tags and Resource Groups for efficient resource management.\nCompleted CloudWatch Workshop, created Dashboards, Alarms, and Metric Filters.\nDeployed FCJ Management application using Auto Scaling Group, ensuring high availability and automatic scaling.\nCompleted Lightsail Workshop – Cost Optimization, gaining knowledge on cost-efficient small workloads.\n"},{"uri":"https://whopqa.github.io/AWS_Intern/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Understand advanced AWS storage services: S3, Storage Gateway, AWS Backup, VM Import/Export, Amazon FSx. Practice hybrid storage integration between on-premises and AWS. Learn how to host and manage static websites on Amazon S3. Gain hands-on experience with backup automation and VM migration workflows. Implement shared Windows file storage using Amazon FSx. Tasks to carry out this week: Day Tasks Start Date Completion Date Reference Material 2 - Completed Module 4 theory: + S3 concepts + Storage Classes (Standard, IA, Intelligent-Tiering, One Zone IA, Glacier) + CORS in S3 + Versioning + Object Key \u0026amp; Performance + Snow Family + Storage Gateway + AWS Backup 22/9/2025 22/9/2025 AWS Docs 3 - Completed Lab 13 (AWS Backup): + Create backup vault + Create backup plan + Assign resources + Run backup jobs + Restore resources + Configure SNS notifications - Completed Lab 14 (VM Import/Export): + Prepare VM OVA/OVF/VMDK + Upload to S3 + Create vmimport role + Import VM → AMI + Launch EC2 from AMI + Export instance if needed 23/9/2025 23/9/2025 AWS Docs 4 - Completed Lab 24 (File Storage Gateway): + Create \u0026amp; activate Storage Gateway + Connect S3 bucket as backend + Create NFS/SMB file share + Install Storage Gateway client on on-prem machine + Mount file share + Test file sync 24/9/2025 24/9/2025 AWS Docs 5 - Completed Lab 57 (Amazon S3 Static Website): + Create S3 bucket + Enable Static Website Hosting + Disable Block Public Access (controlled) + Add Bucket Policy for public read + Upload website files + Test website endpoint + Enable Transfer Acceleration + Enable Versioning + Copy/Move objects + Replicate objects cross-region + Clean up resources 25/9/2025 25/9/2025 AWS Docs 6 - Completed Lab 25 (Amazon FSx for Windows): + Prepare Windows environment \u0026amp; VPC + Create FSx file system + Connect via SMB + Map network drive + Create/read/write files + Run performance tests + Enable Data Deduplication + Enable Shadow Copy + Manage user sessions \u0026amp; file locks + Apply User Quota + Enable Continuous Access + Increase throughput capacity + Expand storage capacity + Clean up FSx resources 26/9/2025 26/9/2025 AWS Docs Week 3 Achievements: Gained strong understanding of advanced S3 features:\nStorage classes, versioning, CORS, performance optimization. Hybrid data transfer options via Snow Family. Completed AWS Backup automation workflow:\nBackup vault, plan, lifecycle, SNS notification, restore. Understood full VM migration cycle using VM Import/Export:\nPreparing VM images, uploading to S3, importing to AMI, exporting back. Successfully deployed File Storage Gateway:\nHybrid NFS/SMB sharing between on-premises and AWS S3. Built and optimized a static website hosted on S3:\nPublic access control, versioning, acceleration, replication. Implemented FSx for Windows as shared storage:\nSMB file sharing, deduplication, shadow copy, quotas, continuous access. Strengthened knowledge about hybrid storage, backup strategies, and enterprise-grade file systems on AWS.\n"},{"uri":"https://whopqa.github.io/AWS_Intern/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Gain solid knowledge of IAM, Shared Responsibility Model, Organizations, Identity Center, KMS, and Security Hub. Deeply understand user/group management, policies, roles, permission boundaries, and IAM conditions. Complete hands-on labs related to access control, permission restriction, EC2 IAM roles, and AWS account security. Strengthen operational security skills: Security Hub, CloudTrail, KMS encryption, and Lambda automation. Tasks to carry out this week: Day Tasks Start Date Completion Date Reference Material Mon - Completed Lab 02 – IAM User \u0026amp; Group:\n+ Create Users, Groups, attach policies\n+ Practice Switch Role \u0026amp; temporary permissions\n- Completed Lab 44:\n+ Create Groups and attach IAM Policies\n+ Create Users, assign policy \u0026amp; Access Key\n+ Configure IAM Role \u0026amp; Conditions (IP, encryption, time-based) 29/09/2025 29/09/2025 IAM Labs Tue - Completed Module 4 theory:\n+ Shared Responsibility Model\n+ IAM Users, Roles, Policies\n+ Cognito for identity management\n+ AWS Organizations \u0026amp; SCP\n+ IAM Identity Center (SSO)\n+ KMS Encryption \u0026amp; Key Management\n+ Security Hub fundamentals 30/09/2025 30/09/2025 AWS Academy Module 4 Wed - Completed Lab 48 – EC2 with IAM Role:\n+ Attach IAM Role to EC2\n+ EC2 receives temporary credentials via STS\n+ App accesses S3 without Access Keys\n- Completed Lab 30 – Permission Boundary:\n+ Create boundary policy\n+ Create restricted IAM user\n+ Validate restricted access\n- Completed Lab 28 – EC2 Tag-based Permissions:\n+ Restrict EC2 access using Resource Tags\n+ Create EC2 Admin role\n+ Validate tag-based access control\n- Completed Lab 18 – Security Hub:\n+ Enable Security Hub\n+ Review CIS/AWS compliance results\n+ Analyze GuardDuty/Inspector/Macie findings 01/10/2025 01/10/2025 IAM \u0026amp; Security Labs Thu - Lab 22 – Lambda auto start/stop EC2 for cost optimization\n+ Create Lambda function\n+ Attach EventBridge Scheduler\n- Lab 33 – S3 Encryption + KMS:\n+ Enable KMS CMK encryption for S3\n+ CloudTrail logs for key usage\n+ Query data using Athena\n- Lab 12 – IAM Identity Center:\n+ Manage SSO for multi-account setup\n+ Assign least-privilege roles\n+ Apply Zero Trust access model 02/10/2025 02/10/2025 AWS Security Workshops Fri - Translated Blog 1: Improve cloud budget accuracy with AWS Budgets new features\n- Translated Blog 2: Increase engagement with localized content using Amazon Bedrock Flows 03/10/2025 03/10/2025 AWS Blog Week 4 Achievements: 1. Completed all Module 4 theory Clearly understand the Shared Responsibility Model. Master IAM Users, Groups, Policies, Roles, and access control mechanisms. Understand how Organizations, SCP, and Identity Center manage multi-account environments. Learn how KMS manages encryption keys across AWS services. Understand Security Hub and its compliance standards (CIS, AWS best practices). 2. Completed 8 critical labs on IAM, EC2, and Security Fully mastered User/Group/Policy management and access control. Know how to attach IAM Roles to EC2 and use STS temporary credentials securely. Apply Permission Boundaries to prevent privilege escalation. Enforce EC2 access control through Resource Tags. Enable and analyze Security Hub findings for compliance and security posture. 3. Cost optimization \u0026amp; data security Automate EC2 start/stop using Lambda + EventBridge to reduce costs. Enable S3 encryption using KMS and audit key usage through CloudTrail. Query S3 data securely with Athena without downloading raw files. 4. Master AWS Identity Center Manage SSO across multiple AWS accounts. Implement least privilege and Zero Trust principles. Centralize authentication \u0026amp; authorization for better governance. 5. Completed translations of 2 AWS blogs Deep understanding of AWS Budgets enhancements. Understand how Bedrock Flows boost localized content engagement. Summary of Week 4: Week 4 focuses on security, access management, and cost optimization, especially around IAM, Security Hub, KMS, and Lambda automation.\nCompleted all theory and advanced labs, building a strong foundation for enterprise-level AWS operations and governance.\n"},{"uri":"https://whopqa.github.io/AWS_Intern/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Complete all theoretical content in Module 5: Shared Responsibility, IAM, Cognito, Organizations, Identity Center, and KMS. Strengthen foundational knowledge of Databases: PK/FK, Index, Partition, Execution Plan, Log, Buffer. Understand the Data Lake architecture and how to use Athena, Glue, and QuickSight for analytics. Practice building a Data Lake and deploying an application using Amazon RDS. Improve AWS documentation reading skills through translating BMW Blog 3. Tasks to carry out this week: Day Tasks Start Date Completion Date Reference Material Mon - Completed Module 5 theory:\n+ Shared Responsibility Model\n+ IAM: Users, Roles, Policies\n+ Amazon Cognito\n+ AWS Organizations + SCP\n+ AWS Identity Center (SSO)\n+ AWS KMS\n- Translated Blog 3: BMW – Generative AI for Cloud Incident Analysis 6/10/2025 6/10/2025 https://docs.google.com/document/d/1HuxyNcXIRpHRY_IEPkBqbSQ4jgbWUMSoetDqe1IPd2o/edit?usp=sharing Tue - Studied Database Concepts:\n+ Database, Session\n+ Primary Key, Foreign Key\n+ Index, Partition\n+ Execution Plan, DB Log, Buffer\n- Differentiated RDBMS vs NoSQL\n- Reviewed basic SQL commands 7/10/2025 7/10/2025 RDS Docs Wed - Studied Data Analytics topics:\n+ Data Lake: storing raw data for analytics\n+ Amazon Athena: SQL querying on S3\n+ AWS Glue: fully managed ETL\n+ Amazon QuickSight: BI \u0026amp; dashboards 8/10/2025 8/10/2025 Analytics Docs Thu - Completed Lab 35 – Data Lake on AWS 9/10/2025 9/10/2025 Data Lake Lab Fri - Completed Lab 05 – Deploying an Application with RDS:\n+ Created VPC + Private Subnets\n+ Created DB Subnet Group (≥ 2 AZs)\n+ Launched RDS Instance: engine, compute, storage, backup\n+ Configured security: Security Group, Parameter Group, KMS Encryption\n+ Connected application via RDS Endpoint\n+ Set up HA \u0026amp; scaling: Multi-AZ + Read Replicas\n+ Monitoring using CloudWatch \u0026amp; Performance Insights\n+ Tested failover \u0026amp; performance optimization 10/10/2025 10/10/2025 RDS Lab Week 5 Achievements : 1. Completed all theoretical content in Module 5 Clearly understood the AWS–Customer Shared Responsibility boundary. Mastered IAM, Cognito, Organizations, Identity Center, and KMS. Understood SSO and enterprise security models on AWS. 2. Strengthened foundational database knowledge Understood PK/FK, Indexes, Partitioning, and query optimization via Execution Plans. Learned how Logs and Buffers operate in RDBMS systems. Understood the differences between RDBMS and NoSQL. Reviewed and reinforced SQL fundamentals. 3. Gained solid understanding of Data Lake architecture Learned how Data Lakes store raw data for analytics. Used Athena to query S3 data with SQL. Used Glue for ETL and metadata management. Used QuickSight for visualization and BI dashboards. 4. Completed two major hands-on labs Lab 35: Built a complete Data Lake solution. Lab 05: Deployed an application with RDS using HA, encryption, and full monitoring. 5. Improved technical reading and translation skills Translated BMW Blog 3 and gained insights into how enterprises use GenAI for cloud incident analysis. Week 5 Summary: Week 5 focused on security, identity, databases, and data analytics, combining theory with hands-on labs.\nCompleting Module 5, the Data Lake lab, and the RDS deployment lab built a strong foundation for designing AWS systems that are:\nsecure – scalable – and data-driven.\n"},{"uri":"https://whopqa.github.io/AWS_Intern/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Complete all theory content in Module 6 (Database Concepts, RDS/Aurora, Redshift, ElastiCache). Research AI chatbot architecture on AWS. Install and use Amazon Q CLI. Attend the “Data Science on AWS” workshop. Start drafting cloud architectures using Amazon Q CLI. Tasks to carry out this week: Day Tasks Start Date Completion Date References Meterial 2 Complete Module 6 theory - Redshift: Data warehouse, OLAP, MPP, columnar storage - ElastiCache: Redis/Memcached, reduce latency, session caching - Database Concepts: OLTP vs OLAP, SQL vs NoSQL, ACID vs BASE, vertical/horizontal scaling, sharding, replication 12/10/2025 12/10/2025 AWS StudyGroup 3 Install Amazon Q CLI on VMWare 13/10/2025 13/10/2025 AWS Docs 4 Research blogs and analyze code - Read “Building AI Chatbots using Amazon Lex \u0026amp; Amazon Kendra” and “Guidance for Conversational Chatbots Using RAG on AWS” - LangChain Agent + Kendra indexing - Store session data using DynamoDB - RAG with Bedrock 14/10/2025 14/10/2025 AWS Blogs 5 Attend Workshop: “Data Science on AWS” - Take notes on data analytics services, and AI/ML services 15/10/2025 15/10/2025 AWS Event 6 Draft initial architecture diagrams using Amazon Q CLI 16/10/2025 16/10/2025 Pre-Architecture Week 6 Achievements: Gained strong understanding of Module 6:\nRedshift: OLAP, MPP, columnar storage, suitable for BI \u0026amp; big data analytics. ElastiCache: Redis/Memcached, caching sessions, offloading DB load, increasing throughput. Database Concepts: OLTP/OLAP, SQL/NoSQL, ACID vs BASE, scaling strategies. Installed and tested Amazon Q CLI:\nCreated Q CLI projects Generated architecture diagrams Executed basic Q Builder commands Conducted deep research on AI chatbot architecture on AWS:\nAmazon Lex for conversational interface Kendra for multi-source data indexing LangChain agent workflow DynamoDB for session storage Bedrock for RAG pipelines Attended Data Science on AWS Workshop:\nLearned full ETL → Transform → Load → ML Training → Deployment pipeline\nUnderstood key AWS data and ML services: S3, Glue, Redshift, Athena, SageMaker\nDrafted initial cloud architectures using Amazon Q CLI:\nChatbot architecture Data pipeline architecture Basic RAG architecture on AWS "},{"uri":"https://whopqa.github.io/AWS_Intern/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Read and analyze AI-related blogs on AWS. Select an architectural baseline from an AWS blog to develop the chatbot. Build the initial project direction for the chatbot. Deploy and test the sample Text-to-SQL Chatbot. Conduct team meeting and summarize optimization strategies (cache, RBAC, scaling, etc.). Tasks to carry out this week: Day Tasks Start Date Completion Date Reference Material 2 Read \u0026amp; review AI blogs - AWS Knowledge Hub - Summarize AI knowledge \u0026amp; best practices 20/10/2025 20/10/2025 AWS Knowledge Hub 3 Select baseline blog: - “Build an AI-powered Text-to-SQL Chatbot” - Study code, architecture, workflow - Analyze technologies: Bedrock, Aurora, SQL generation 21/10/2025 21/10/2025 AWS Blog 4 Create initial chatbot Direction - Write Direction document on Notion - Define pipeline, roles, architecture, bot limitations - Specify interaction rules 22/10/2025 22/10/2025 Direction 5 Deploy sample blog project: “Build an AI-powered Text-to-SQL Chatbot” - Run end-to-end tests - Document issues \u0026amp; optimization ideas 23/10/2025 23/10/2025 AWS Blog 6 Team meeting \u0026amp; notes on Notion Meeting agenda includes: 1. Caching mechanism \u0026amp; bot optimization 2. RBAC access control design 3. Data flow RDS → S3 archive 4. Scaling: RDS, UI, Lex + Translate 24/10/2025 24/10/2025 Internal Meeting Week 7 Achievements: Reviewed AWS Knowledge Hub:\nSummarized common AI chatbot workflows. Noted key AWS services: Bedrock, RDS, S3, caching layer. Selected baseline from “Build an AI-powered Text-to-SQL Chatbot”:\nUnderstood the Text-to-SQL architecture. Captured pipeline: embedding → SQL generation → execution. Completed Direction document on Notion:\nDefined feature scope. Established strategies to prevent incorrect bot actions. Designed role-based flow: member / consultant. Set safe rules for SELECT/UPDATE/DELETE. Successfully deployed the sample model:\nPerformed real-world testing. Documented improvement points. Meeting summary:\n1. Cache \u0026amp; bot optimization\nCache frequent questions to reduce database load. Prevent bot from having unrestricted modification permissions. 2. RBAC – Access control \u0026amp; security\nBot operates using credentials tied to user role. SELECT allowed broadly; UPDATE/DELETE requires strict role \u0026amp; logic checks. 3. Data flow \u0026amp; storage\nArchive RDS data to S3 for analytics and dashboards. 4. Scaling\nRDS: read replicas, Multi-AZ, autoscaling. UI scaling. Lex + Translate for Vietnamese support (Translate free 2M characters/month in the first year). "},{"uri":"https://whopqa.github.io/AWS_Intern/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Finalize the preliminary components and technologies to be used in the project. Review and improve the architecture and technology choices. Study AWS theory through videos and documentation. Systematize knowledge of core AWS services. Practice for AWS Practitioner \u0026amp; AWS SAA-C03 certifications. Complete the midterm exam Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Finalize preliminary components \u0026amp; key technologies - Identify core services and modules - Evaluate areas that need improvement 27/10/2025 27/10/2025 Internal project architecture 3 Review AI/AWS theory videos on YouTube 28/10/2025 28/10/2025 YouTube: AWS Tutorials 4 Systematize AWS knowledge - EC2, RDS, S3, IAM, Lambda, VPC - Review using AI tools (ChatGPT/Claude) 29/10/2025 29/10/2025 AWS Docs 5 Theory review \u0026amp; exam practice - Quizlet: AWS Hotfix V10 - Practice Exam AWS Practitioner - Practice Exam AWS SAA-C03 30/10/2025 30/10/2025 Quizlet, AWS Exam Guide 6 Midterm exam - Select Top 10 highest-scoring candidates (Round 1) 31/10/2025 31/10/2025 Internal exam results Week 8 Achievements: 1. Finalizing preliminary project components Identified the core AWS + AI services:\n1.1. RAG Module – in progress\nChallenge: understanding \u0026amp; extracting Vietnamese intent. Solution: Lex → Lambda (acting as RAG) → DynamoDB. Intent = Customer Support → retrieve RAG from DynamoDB. Intent = Scheduling → call Bedrock to generate SQL. 1.2. Scheduling Module\nChallenge: safely executing insert/update/delete SQL queries on the appointment table. Tech stack: RDS + pgvector. Cost optimization: consider EventBridge to start/stop RDS. 1.3. User UI Flow\nLex → Lambda → Amazon Translate → intent detection → RAG or Scheduling. 1.4. Admin UI\nWeb dashboard (weekly/monthly/yearly analytics) → S3 + CloudFront + Route53 → admin-level DB actions. Combine Athena + Glue for analytics and reporting. 1.5. Memory System (Cache) – DynamoDB\nStores: session history, RAG results, SQL results, Bedrock outputs. Implementation: Lambda trigger on DB changes → invalidate cache; TTL for auto-expiry (e.g., 1 hour). 1.6. Storage\nPeriodically archive appointment data to S3 for analytics/revenue tracking. 2. Technical review \u0026amp; improvements Documented optimization points before building. Evaluated all architecture and technology decisions. 3. AWS Services Review Strengthened understanding of EC2, RDS, IAM, S3, Networking. Used AI to reinforce key concepts. 4. AWS Certification Practice Completed Quizlet Hotfix V10. Practiced AWS Practitioner and SAA-C03 exams. 5. Midterm Exam Completed the exam. Made it into the Top 10 highest-scoring candidates (Round 1). Week 8 Summary: Week 8 focused on refining the project architecture and reviewing AWS fundamentals.\nThe main modules (RAG, Scheduling, UI, Cache, Storage) and their associated technologies were clearly identified.\nAt the same time, AWS knowledge was reinforced through exam practice, achieving solid results in the midterm assessment.\n"},{"uri":"https://whopqa.github.io/AWS_Intern/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Finalize and align the overall system architecture. Evaluate technology selections: Lex, Translate, Bedrock, Webhook. Optimize system cost and deployment model. Standardize the proposal: problem statement, components, dataflow. Submit Proposal – Phase 1. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Team meeting on the updated project approach - Proposed switching from Amazon Lex + Translate to Custom Webhook + Bedrock Claude - Reason: Lex does not support Vietnamese, Claude handles Vietnamese much better 11/03/2025 11/03/2025 Internal Documentation 3 - Finalize the overall system architecture on draw.io - Upload the architecture diagram to Google Drive 11/04/2025 11/04/2025 Proposal, Architecture 4 - Continue improving the architecture + Reduce unnecessary VPC Endpoints to optimize cost + Organize Lambda functions based on security \u0026amp; performance requirements (inside/outside VPC) + Number and standardize all dataflows 11/05/2025 11/05/2025 Proposal, Architecture 5 - Calculate pricing for the entire system - Review and refine the Proposal: + Problem objectives + Main system components \u0026amp; their roles + Explanation of the dataflow 11/06/2025 11/06/2025 AWS Pricing 6 - Finalize the Proposal - Submit Proposal – Phase 1 11/07/2025 11/07/2025 Proposal, Architecture Week 9 Achievements: Aligned on the new technical approach:\nRemoved Lex + Translate from the architecture. Switched to Custom Webhook + Bedrock Claude for Vietnamese conversational workflows. Simplified the pipeline and improved response accuracy. Completed the overall system architecture:\nUpdated the entire flow: Webhook → Bedrock → Lambda → Database. Optimized Lambda placement (inside or outside VPC). Numbered and standardized all data pipelines. Cost optimization:\nReduced unnecessary VPC Endpoints. Reviewed projected cost for RDS, S3, Lambda, Bedrock, API Gateway. Pricing calculation \u0026amp; cost report created.\nCompleted Proposal – Phase 1:\nClear and well-defined problem objectives. Detailed description of system components and roles. Updated and standardized dataflow explanation. Reformatted and finalized content for the team submission. "},{"uri":"https://whopqa.github.io/AWS_Intern/1-worklog/","title":"Worklog","tags":[],"description":"","content":"This worklog documents my 13-week journey during the AWS internship program, covering fundamental AWS services, hands-on labs, AI chatbot architecture design, and full-stack development using AWS CDK. Below is a weekly breakdown of key achievements and technical milestones:\nWeek 1: Getting familiar with AWS, Hugo, VPC and basic Networking\nWeek 2: VPC, EC2, Site-to-Site VPN, Transit Gateway and Auto Scaling practice\nWeek 3: AWS Storage: S3, Backup, VM Import/Export, Storage Gateway and FSx\nWeek 4: IAM, Security Hub, Lambda automation and KMS Encryption\nWeek 5: Database concepts, RDS, Data Lake with Athena and Glue\nWeek 6: Redshift, ElastiCache, Amazon Q CLI and AI Chatbot research\nWeek 7: Selecting AI chatbot baseline architecture and deploying Text-to-SQL prototype\nWeek 8: AWS Practitioner and SAA-C03 exam preparation, midterm evaluation\nWeek 9: Finalizing architecture, pricing calculation and Proposal submission\nWeek 10: Learning CDK, team task allocation and deploying Text2SQL, Webhook Stack\nWeek 11: Deploying Webhook Stack, OTP authentication via SES and session management\nWeek 12: Chatbot system restructuring and appointment scheduling flow development.\nWeek 13: Finalizing appointment scheduling flow, handling throttling, optimizing prompts and submiting project\n"},{"uri":"https://whopqa.github.io/AWS_Intern/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Key Takeaways from \u0026ldquo;AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS\u0026rdquo; Event Overview This session provided an in-depth exploration of AWS\u0026rsquo;s AI/ML/GenAI ecosystem and practical approaches to implementing these technologies in production environments. The event brought together cloud practitioners, AI enthusiasts, and industry experts to share insights on leveraging AWS\u0026rsquo;s comprehensive AI portfolio for building next-generation intelligent applications.\nSpeakers Lam Tuan Kiet – Sr DevOps Engineer, FPT Software Danh Hoang Hieu Nghi – AI Engineer, Renova Cloud Dinh Le Hoang Anh – Cloud Engineer Trainee, First Cloud AI Journey Van Hoang Kha – Community Builder Main Topics Covered 1. Building with Amazon Bedrock\u0026rsquo;s Generative AI Platform Understanding Foundation Models:\nAmazon Bedrock provides access to pre-trained foundation models from leading AI companies like Anthropic, OpenAI, and Meta. These models can be fine-tuned for specific use cases without requiring extensive training infrastructure. This democratizes AI development by eliminating the need for specialized ML expertise and expensive GPU resources, allowing teams to focus on solving business problems rather than managing infrastructure.\nEffective Prompt Engineering Approaches:\nDifferent prompting methods can significantly impact model performance:\nZero-shot prompting: Direct task instruction without examples, suitable for straightforward queries. Few-shot prompting: Including sample inputs and outputs to guide model behavior. Chain-of-Thought prompting: Requesting step-by-step reasoning to improve complex problem-solving accuracy. Retrieval Augmented Generation (RAG) Architecture:\nRAG enhances AI responses by combining model capabilities with external knowledge bases, addressing the challenge of hallucinations and keeping AI systems up-to-date with current information:\nRetrieval phase: Searches relevant information from document repositories using vector similarity. Augmentation phase: Enriches the prompt with retrieved context, providing grounding for the model. Generation phase: Produces informed, accurate responses based on both model knowledge and external data. Use cases: Intelligent chatbots, semantic search systems, automated content summarization, and domain-specific question answering. Benefits: Reduces hallucinations, enables real-time knowledge updates without retraining, and provides source attribution for transparency. Amazon Titan Embeddings:\nThis specialized model converts text into numerical vector representations (embeddings), enabling semantic similarity matching and powering RAG implementations with multilingual capabilities. Unlike traditional keyword matching, embeddings capture the semantic meaning of text, allowing systems to find conceptually similar content even when exact words don\u0026rsquo;t match. This is particularly valuable for cross-lingual applications and understanding user intent.\nPre-built AWS AI Services: Production-ready APIs for common AI tasks:\nRekognition – Visual content analysis Translate – Multi-language translation Textract – Intelligent document processing Transcribe – Audio transcription Polly – Voice synthesis Comprehend – Text analysis and NLP Kendra – Enterprise search Lookout – Anomaly detection Personalize – Personalized recommendations Live Demonstration:\nThe AMZPhoto application showcased practical facial recognition implementation using AWS AI services. This hands-on demo illustrated how to integrate multiple AWS services (Rekognition, S3, Lambda) into a cohesive application, demonstrating real-world architecture patterns and best practices for building production-ready AI solutions.\n2. Scaling AI Agents with Amazon Bedrock AgentCore AgentCore provides enterprise-grade infrastructure for deploying autonomous AI agents that can perform complex, multi-step tasks:\nSecure workflow execution and scaling capabilities with automatic load balancing Persistent memory management for stateful interactions across sessions Granular access control and security policies ensuring compliance with enterprise requirements Built-in integrations: Browser Tool, Code Interpreter, Memory Store, and custom function calling Comprehensive monitoring and audit trails for debugging and governance Compatibility with major frameworks: CrewAI, LangGraph, LlamaIndex, OpenAI Agents SDK Support for multi-agent collaboration and orchestration patterns Key Learning Points Centralized AI development with Bedrock: Single platform for accessing diverse foundation models simplifies development and reduces vendor lock-in risks. Combining prompts and RAG: These techniques are essential for building context-aware AI applications that can handle domain-specific knowledge. Vector embeddings enable semantic understanding: Titan Embeddings improve search relevance and information retrieval beyond simple keyword matching. Leverage managed services: AWS\u0026rsquo;s pre-built AI tools significantly reduce time-to-market and operational overhead. AgentCore for production agents: Handles operational complexity including scaling, security, and observability, allowing teams to focus on agent logic. Cost optimization strategies: Understanding pricing models and implementing caching strategies can significantly reduce AI inference costs. Security and compliance: AWS\u0026rsquo;s AI services are designed with enterprise security requirements in mind, including data privacy and regional compliance. Real-World Applications The RAG architecture and AgentCore framework are directly applicable to our planned GenAI initiatives, providing a blueprint for implementation. We can leverage these patterns for building internal knowledge assistants and customer-facing chatbots. Deep familiarity with AWS AI service portfolio enables better architectural decisions and faster prototyping. Understanding service capabilities helps in selecting the right tool for each use case. Prompt engineering techniques can immediately improve our AI-powered feature quality and reliability, reducing the need for extensive fine-tuning. The embedding-based search approach can enhance our existing search functionality, improving user experience and content discovery. Multi-agent patterns demonstrated with AgentCore can be applied to automate complex workflows that currently require manual intervention. Personal Event Highlights The \u0026ldquo;GenAI-powered App-DB Modernization\u0026rdquo; workshop was an enriching learning experience, demonstrating practical strategies for modernizing legacy systems with cutting-edge AI and database technologies. The hands-on format encouraged active participation and knowledge sharing among attendees. Notable moments included:\nAchieving top 5 in the closing Kahoot competition and meeting the speakers for photos, providing an opportunity to discuss implementation challenges and best practices. Forming team \u0026ldquo;Mèo Cam Đeo Khăn\u0026rdquo; (Orange Cat with Scarf) by merging talent from \u0026ldquo;The Ballers\u0026rdquo; and \u0026ldquo;Vinhomies\u0026rdquo; groups, fostering collaboration and networking. Engaging in technical discussions with fellow participants about real-world AI implementation challenges and solutions. Gaining valuable connections within the AWS community that will facilitate future knowledge exchange and collaboration opportunities. Event Gallery "},{"uri":"https://whopqa.github.io/AWS_Intern/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Key Takeaways from \u0026ldquo;AWS Cloud Mastery Series #2 – DevOps on AWS\u0026rdquo; Event Overview This comprehensive session explored modern DevOps practices on AWS, bringing together industry practitioners and community builders to share hands-on experiences with infrastructure automation, continuous delivery, containerization, and observability. The event emphasized practical implementation strategies that teams can adopt immediately to improve their development workflows and operational efficiency.\nSpeakers Truong Quang Tinh – AWS Community Builder, Platform Engineer (TymeX) Bao Huynh – AWS Community Builder Nguyen Khanh Phuc Thinh – AWS Community Builder Tran Dai Vi – AWS Community Builder Huynh Hoang Long – AWS Community Builder Pham Hoang Quy – AWS Community Builder Nghiem Le – AWS Community Builder Dinh Le Hoang Anh – Cloud Engineer Trainee, First Cloud AI Journey Main Topics Covered 1. Embracing the DevOps Mindset The speakers emphasized that DevOps represents a cultural shift rather than merely a job role. Success in DevOps comes from cultivating specific practices and mindsets:\nCore DevOps Principles:\nAutomation-first approach: Eliminate manual, repetitive tasks to reduce human error and increase velocity Cross-functional collaboration: Break down silos between development, operations, and security teams Continuous learning culture: Embrace experimentation, learn from failures, and iterate rapidly Data-driven decision making: Replace assumptions with metrics and observable evidence Common Challenges for Beginners: The speakers shared valuable insights about mistakes many newcomers make:\nTutorial paralysis: Watching endless courses without building actual projects Tool obsession: Focusing on learning every tool rather than mastering fundamentals Comparison trap: Measuring progress against others instead of focusing on personal growth Neglecting soft skills: Underestimating the importance of communication and documentation Practical Advice: Start small with one service or workflow, automate it completely, then expand. The best way to learn DevOps is by solving real problems in real environments.\n2. Infrastructure as Code: Tools and Trade-offs Rather than advocating for a single solution, the speakers provided a nuanced comparison of IaC approaches, helping attendees make informed decisions based on their specific contexts.\nCloudFormation:\nNative AWS integration with zero additional tools required JSON/YAML templates that explicitly define resources Deep AWS service coverage including newest features Best for: Teams exclusively on AWS seeking official support AWS CDK (Cloud Development Kit):\nWrite infrastructure using familiar programming languages (TypeScript, Python, Java, C#) Higher-level abstractions that reduce boilerplate code Type safety and IDE autocomplete improve developer experience Synthesizes to CloudFormation for deployment Best for: Development teams who prefer programmatic infrastructure definition Terraform:\nCloud-agnostic declarative language (HCL) Extensive provider ecosystem beyond just AWS Strong state management and planning capabilities Large community and module registry Best for: Multi-cloud strategies or teams with prior Terraform investment Key Concepts Clarified:\nStacks: Logical grouping of related resources that are managed together Constructs (CDK): Reusable cloud components at various abstraction levels State files: Records of infrastructure managed by tools like Terraform Drift detection: Identifying manual changes that deviate from code definitions Critical Insight: Infrastructure defined as code enables version control, peer review, testing, and reproducibility—benefits impossible with manual console configurations. Even simple IaC is better than elaborate manual setups.\n3. Containerization and Orchestration Strategies The container discussion progressed from fundamentals to production-grade deployment patterns.\nContainer Fundamentals:\nDockerfile anatomy: Understanding layers, caching, and multi-stage builds Image optimization: Techniques for reducing size and improving security Best practices: Immutable infrastructure, minimal base images, security scanning AWS Container Services Deep Dive:\nAmazon ECR (Elastic Container Registry):\nPrivate container image registry with vulnerability scanning Image lifecycle policies for automated cleanup Integration with IAM for access control Cross-region and cross-account replication Amazon ECS (Elastic Container Service):\nAWS-native container orchestration Fargate launch type eliminates server management Tight integration with AWS services (ALB, CloudWatch, Secrets Manager) Simpler learning curve than Kubernetes Best for: Teams wanting container orchestration without Kubernetes complexity Amazon EKS (Elastic Kubernetes Service):\nManaged Kubernetes control plane Full Kubernetes API compatibility Extensive ecosystem of tools and operators Portable across cloud providers Best for: Organizations standardizing on Kubernetes or requiring advanced orchestration features AWS App Runner:\nFully managed service for containerized web apps Automatic scaling and load balancing No infrastructure management required Best for: Simple web applications needing rapid deployment Decision Framework:\nApp Runner → Simple web apps, fastest time to value ECS → AWS-centric workloads, team familiar with AWS services EKS → Complex orchestration needs, Kubernetes expertise, multi-cloud portability 4. Observability and Performance Monitoring The observability segment highlighted that monitoring must be an architectural concern, not an operational afterthought.\nAWS CloudWatch Capabilities:\nMetrics: Collect and track quantitative data from AWS services and applications Logs: Centralized log aggregation with powerful query capabilities (CloudWatch Insights) Dashboards: Visual representations of system health and performance Alarms: Automated notifications and remediation based on thresholds Events/EventBridge: Event-driven automation and workflow triggers AWS X-Ray for Distributed Tracing:\nVisualize request paths through microservices architectures Identify performance bottlenecks and errors in distributed systems Service maps showing dependencies and communication patterns Detailed trace analysis with timing breakdowns Integration with Lambda, ECS, EKS, API Gateway, and more Observability Best Practices:\nInstrument code from day one, not after production issues arise Establish baseline metrics to understand normal behavior Create actionable alerts that require response, not just noise Use distributed tracing for complex, multi-service architectures Implement structured logging for easier querying and analysis The Three Pillars of Observability:\nMetrics: What is happening (quantitative) Logs: Why it\u0026rsquo;s happening (qualitative details) Traces: How requests flow through the system (relationships) Key Learning Points DevOps as culture, not title: Success requires mindset shifts toward automation, collaboration, experimentation, and measurement rather than simply adopting new tools. IaC fundamentals: Code-based infrastructure provides consistency, repeatability, and collaboration benefits that manual processes cannot match. Choose tools based on team strengths and project requirements. Container service selection: Understanding the trade-offs between App Runner, ECS, and EKS enables optimal decisions for different workload types and team capabilities. Observability from inception: Build monitoring, logging, and tracing into architecture design rather than retrofitting after production deployment. Start small, iterate often: Begin with one automated workflow or service, perfect it, then expand—avoid attempting to transform everything simultaneously. Learn by doing: Hands-on project work provides deeper understanding than passive learning through tutorials alone. Real-World Applications The knowledge gained from this workshop can be directly applied to real-world projects, particularly in building modern, scalable applications on AWS.\nExample Implementation: AI-Powered Chatbot Platform\nFuture AI chatbot projects can leverage these DevOps principles for production-ready deployment:\nInfrastructure as Code Implementation:\nUse AWS CDK with TypeScript to define entire infrastructure stack programmatically Version control all infrastructure code for audit trails and rollback capabilities Define resources including Lambda functions, API Gateway, DynamoDB tables, S3 buckets, IAM roles, and CloudWatch alarms Create reusable constructs for common patterns (API endpoints, database tables, Lambda layers) Enable easy environment replication (dev, staging, production) with parameter variations CI/CD Pipeline Architecture:\nSource stage: GitHub integration triggering on pull requests and merges Build stage: CodeBuild compiling code, running unit tests, and security scans Test stage: Automated integration tests against staging environment Deploy stage: Gradual rollout with blue-green deployment pattern Monitoring stage: Automated rollback on error rate threshold breaches Containerization Strategy:\nPackage chatbot backend services as Docker containers for consistent environments Store images in ECR with automated vulnerability scanning Deploy containers using ECS Fargate for serverless container execution Implement auto-scaling based on request volume and response times Observability Implementation:\nCloudWatch metrics tracking conversation success rates, response latency, and error rates X-Ray tracing for end-to-end request visibility through API Gateway, Lambda, and database calls Structured logging for conversation analysis and debugging Custom dashboards providing business metrics (user engagement, topic distribution) Alarms for anomaly detection and automated incident notification Expected Outcomes:\nReduced deployment time from hours to minutes through automation Improved reliability with infrastructure testing and gradual rollouts Enhanced troubleshooting with comprehensive observability Seamless scaling to handle variable user loads Faster feature delivery through streamlined workflows Personal Event Highlights This workshop provided tremendous value by combining theoretical DevOps principles with practical AWS implementation patterns. Unlike purely conceptual sessions, this event featured:\nStrengths of the Event:\nReal-world case studies from speakers\u0026rsquo; production experiences Comparative analysis of tools rather than single-solution advocacy Emphasis on decision-making frameworks over prescriptive rules Balanced coverage across infrastructure, deployment, and operations Interactive Q\u0026amp;A addressing specific attendee challenges Networking and Community: The event facilitated meaningful connections with fellow cloud practitioners facing similar challenges. Discussions during breaks and after sessions revealed common patterns in organizations\u0026rsquo; DevOps journeys, providing validation that many challenges are shared across the industry.\nKnowledge Application: The insights gained will directly influence upcoming projects, particularly around:\nAdopting CDK for new infrastructure projects requiring flexibility Implementing comprehensive observability before production launch Establishing CI/CD pipelines with appropriate testing gates Selecting container services based on complexity requirements Event Gallery "},{"uri":"https://whopqa.github.io/AWS_Intern/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Key Takeaways from \u0026ldquo;AWS Cloud Mastery Series #3: AWS Well-Architected – Security Pillar Workshop\u0026rdquo; Event Overview This comprehensive workshop focused on the Security Pillar of the AWS Well-Architected Framework, bringing together cloud security experts, community leaders, and students to explore enterprise-grade security practices on AWS. The session provided practical guidance on implementing defense-in-depth strategies across multiple security domains, emphasizing real-world scenarios and hands-on demonstrations. The event also highlighted the AWS Cloud Clubs program, showcasing how academic communities can accelerate cloud learning and professional development.\nSpeakers AWS Cloud Club Captains:\nLe Vu Xuan An – AWS Cloud Club Captain, HCMUTE Tran Duc Anh – AWS Cloud Club Captain, SGU Tran Doan Cong Ly – AWS Cloud Club Captain, PTIT Danh Hoang Hieu Nghi – AWS Cloud Club Captain, HUFLIT AWS Community Builders:\nHuynh Hoang Long – AWS Community Builder Dinh Le Hoang Anh – AWS Community Builder Van Hoang Kha – Cloud Security Engineer, AWS Community Builder Tinh Truong – Platform Engineer at TymeX, AWS Community Builder Cloud Engineers \u0026amp; FCJ Members:\nNguyen Tuan Thinh – Cloud Engineer Trainee Nguyen Do Thanh Dat – Cloud Engineer Trainee Thinh Lam – FCJ Member Viet Nguyen – FCJ Member Industry Expert:\nMendel Grabski (Long) – Ex-Head of Security \u0026amp; DevOps, Cloud Security Solution Architect Main Topics Covered 1. AWS Cloud Clubs: Building Student Cloud Communities The workshop opened with an inspiring introduction to the AWS Cloud Clubs program, demonstrating how student-led initiatives are transforming cloud education across Vietnamese universities.\nProgram Overview: The AWS Cloud Clubs create structured learning environments where students can develop cloud expertise through practical, hands-on experiences. These clubs operate at leading universities including HCMUTE, SGU, PTIT, and HUFLIT, each fostering local cloud communities.\nCore Benefits:\nHands-on Learning: Project-based approach where students build real applications on AWS rather than just consuming theoretical content Expert Mentorship: Direct access to guidance from AWS professionals, community builders, and industry practitioners Peer Collaboration: Network building among students passionate about cloud technology, creating study groups and project teams Career Development: Bridge between academic learning and professional requirements, helping students prepare for cloud careers Resource Access: AWS credits, training materials, and certification support for club members Impact and Vision: Cloud Clubs serve as incubators for technical talent, providing students with the skills, credentials, and connections needed to launch successful cloud careers. The collaborative environment accelerates learning through knowledge sharing and mutual support.\n2. Identity \u0026amp; Access Management: Foundation of Security IAM emerged as the cornerstone of AWS security, with speakers emphasizing that proper access control prevents the majority of cloud security incidents.\nFundamental IAM Principles:\nLeast Privilege Access:\nGrant only the minimum permissions required for specific tasks Regularly audit and remove unnecessary permissions Use permission policies that specify exact resources rather than wildcards (*) Implement time-bound access for elevated privileges Root Account Security:\nDelete root access keys immediately after account setup Enable MFA on root account using hardware tokens when possible Use root account only for tasks that explicitly require it Store root credentials in secure, offline locations with restricted access Multi-Account Strategy:\nAWS Organizations: Centralized management for multiple AWS accounts Service Control Policies (SCPs): Organization-level permission boundaries that restrict what even administrators can do AWS Single Sign-On (SSO): Unified access management across all accounts with SAML 2.0 integration Account segregation: Separate production, development, and security workloads for isolation Advanced IAM Concepts:\nPermission Boundaries:\nMaximum permissions that IAM entities can have, regardless of their policies Useful for delegating user creation while preventing privilege escalation Applied to users and roles to enforce organizational security standards Acts as a guardrail even when teams have permission to create policies Multi-Factor Authentication (MFA): The workshop compared two MFA implementations:\nTOTP (Time-based One-Time Password): Software-based authenticators (Google Authenticator, Authy), easier to deploy but vulnerable to phishing FIDO2/WebAuthn: Hardware security keys (YubiKey), phishing-resistant with cryptographic verification, more secure but higher cost Secrets Management Best Practices: AWS Secrets Manager automates credential lifecycle management:\nCreate: Generate new credentials with required complexity Set: Update the secret in AWS and propagate to applications Test: Verify new credentials work before finalizing rotation Finalize: Mark rotation complete and deprecate old credentials Enables automatic rotation (30, 60, or 90 days) Integrates with RDS, DocumentDB, Redshift for database credential rotation Provides audit trail of all secret access through CloudTrail IAM Best Practices Summary:\nUse IAM roles instead of access keys for applications Implement policy conditions to restrict access by IP, time, or MFA Regular access reviews using IAM Access Analyzer Tag-based access control (ABAC) for dynamic, scalable permissions 3. Continuous Monitoring \u0026amp; Threat Detection The monitoring section emphasized proactive security through comprehensive visibility and automated threat detection across AWS environments.\nAWS CloudTrail: The Foundation of Audit Logging\nCloudTrail captures three types of events, each serving different security purposes:\nManagement Events:\nAPI calls that modify AWS resources (CreateInstance, DeleteBucket) Configuration changes to services Account activity (login attempts, key creation) Critical for compliance and forensic investigations Data Events:\nObject-level S3 operations (GetObject, PutObject, DeleteObject) Lambda function invocations with payload details Higher volume but essential for detecting data exfiltration Typically enabled only on sensitive buckets or functions Network Events:\nVPC Flow Logs capturing packet-level metadata DNS query logs for domain resolution tracking Network interface traffic patterns CloudTrail Lake:\nSQL-queryable event data store for advanced analysis Retention up to 7 years for long-term compliance Enables \u0026ldquo;Detection-as-Code\u0026rdquo; by deploying queries via Infrastructure as Code Cross-account and cross-region log aggregation Amazon EventBridge: Automated Response\nEventBridge transforms CloudTrail events into actionable security responses:\nReal-time alerting: Notify security teams immediately when sensitive operations occur Automated remediation: Trigger Lambda functions to reverse unauthorized changes Cross-account routing: Centralize security events from all organizational accounts Integration ecosystem: Connect to SNS, SQS, Step Functions, or external SIEMs Example Use Cases:\nDetect and respond to security group changes that open sensitive ports Alert when root account credentials are used Automatically disable access keys that haven\u0026rsquo;t been rotated Quarantine S3 buckets that become publicly accessible 4. Amazon GuardDuty: Intelligent Threat Detection GuardDuty provides continuous threat intelligence without requiring agent deployment or log management.\nCore Detection Capabilities:\nThreat Intelligence Analysis:\nExamines CloudTrail events, VPC Flow Logs, and DNS queries Compares activity against known malicious IP databases, domains, and patterns Machine learning baselines to detect anomalous behavior specific to your environment Detection Categories:\nCredential compromise: Unusual API calls from new locations, impossible travel scenarios Instance compromise: Cryptocurrency mining, botnet communication, data exfiltration Account compromise: Logging disabled, GuardDuty disabled, unusual instance launches Network anomalies: Port scanning, DDoS participation, suspicious DNS queries Advanced Protection Features:\nS3 Malware Protection:\nScans objects for malware when uploaded to protected buckets Integrates with S3 Event Notifications for automatic scanning triggers Quarantine infected objects using S3 Object Lock or Lambda-based movement EKS Audit Log Analysis:\nMonitors Kubernetes API server logs for suspicious activity Detects privilege escalation, anonymous access, and credential access attempts No impact on cluster performance—analysis happens in GuardDuty service RDS Protection:\nAnalyzes RDS and Aurora login activity for anomalies Detects brute force attacks, unusual access patterns, and suspicious queries Works without database agents or performance overhead Lambda Network Protection:\nInspects network activity from Lambda functions Identifies outbound connections to malicious domains or IPs Detects data exfiltration through serverless functions Runtime Monitoring (GuardDuty Agent):\nDeployed on EC2 instances and ECS containers Monitors file system, process execution, and network connections Detects runtime threats like privilege escalation and suspicious process execution Compliance Alignment: GuardDuty findings map to security frameworks:\nAWS Foundational Security Best Practices CIS AWS Foundations Benchmark PCI DSS for payment card industry compliance ISO 27001 control alignment Response Integration:\nEventBridge integration for automated remediation Security Hub aggregation for centralized dashboard Third-party SIEM integration via S3 export or streaming 5. Infrastructure Protection: Network Security The infrastructure security module covered layering defenses from perimeter to application level.\nUnderstanding Attack Vectors:\nAttack Classifications:\nIngress attacks: External threats attempting to penetrate VPC boundaries (DDoS, exploitation) Egress threats: Data exfiltration or command-and-control communication from compromised resources Insider threats: Malicious or negligent actions from users with legitimate access Network Security Layers:\nSecurity Groups (Stateful Firewall):\nInstance-level protection acting as virtual firewalls Stateful: Return traffic automatically allowed Only allow rules (deny by default) Best practice: Reference other security groups instead of IP ranges for dynamic scaling Network Access Control Lists (NACLs - Stateless Firewall):\nSubnet-level protection Stateless: Require explicit rules for both directions Support both allow and deny rules with rule number precedence Useful for blocking known malicious IPs at subnet boundary AWS Network Firewall: Modern, managed firewall service for VPC protection:\nStateful inspection: Deep packet inspection with protocol detection Intrusion Prevention System (IPS): Signature-based threat detection using Suricata-compatible rules Domain filtering: Allow/deny traffic based on domain names with TLS inspection Traffic logging: Detailed flow and alert logs for security analysis Use Cases:\nEgress filtering: Control outbound internet access from private subnets East-West filtering: Segment VPCs and prevent lateral movement Centralized architecture: Hub-and-spoke topology with Transit Gateway Compliance: Meet regulatory requirements for firewall logging and inspection Route 53 Resolver:\nDNS firewall for hybrid environments Block malicious domains at DNS resolution layer Forward queries between on-premises and AWS Integrate with GuardDuty threat intelligence for automatic blocking Integration Strategy: Combining services creates defense-in-depth:\nGuardDuty detects threats → EventBridge triggers → Lambda updates Network Firewall rules Automated threat blocking without manual intervention Centralized threat intelligence sharing across accounts 6. Data Protection \u0026amp; Encryption Data security encompasses protecting information at rest, in transit, and during processing.\nAWS Key Management Service (KMS) Architecture:\nKey Hierarchy:\nCustomer Master Keys (CMK): Never leave AWS in plaintext, used only for encryption operations Data Encryption Keys (DEK): Generated by KMS, used to encrypt actual data Envelope encryption: CMK encrypts DEK, DEK encrypts data—limits exposure of master keys KMS Features:\nAutomatic key rotation: Annual rotation for AWS-managed and customer-managed keys IAM integration: Fine-grained access control with policy conditions Audit logging: All key usage logged in CloudTrail for compliance Multi-region keys: Replicate keys across regions for disaster recovery Encryption Best Practices:\nEncrypting Data at Rest:\nS3: Server-side encryption (SSE-S3, SSE-KMS, SSE-C) or client-side encryption EBS: Encryption enabled at volume creation, transparent to applications RDS: Encryption for database and backups, cannot be enabled post-creation DynamoDB: Encryption by default with AWS-owned or customer-managed keys Enforcing Encryption:\nS3 bucket policies: Deny uploads without encryption headers DynamoDB condition keys: Require requests to use encrypted connection IAM policies: Restrict KMS key usage to specific services or principals Certificate Management:\nAWS Certificate Manager (ACM):\nFree public SSL/TLS certificates for AWS services Automatic renewal eliminates certificate expiration issues Integration with CloudFront, ALB, API Gateway for HTTPS termination Private CA for internal certificates and custom hierarchies Database TLS Configuration:\nRDS SSL/TLS: Force encrypted connections with parameter group settings Client verification: Validate server certificates to prevent MITM attacks Certificate rotation: Automatic handling by AWS for RDS-managed certificates AWS Secrets Manager: Beyond basic credential storage:\nAutomatic rotation: Lambda-based rotation for supported services Fine-grained access: Resource-based policies for cross-account access Versioning: Maintain multiple versions during rotation for zero-downtime Integration: Native support from SDKs, CLI, and services like ECS 7. Incident Response: Prepare, Detect, Respond The final module outlined structured approaches to security incident management.\nPrevention Strategies:\nSecurity by Design:\nTemporary credentials: Use IAM roles and STS instead of long-lived access keys Private networking: Place sensitive services in private subnets without internet access Least privilege: Regular permission audits using IAM Access Analyzer recommendations Infrastructure as Code: Enforce consistent security configurations through code review Change control: Implement approval workflows for production changes (PR reviews + pipeline approvals) Preventive Controls:\nS3 Block Public Access enabled by default at organization level SCPs preventing users from disabling security services AWS Config rules for automated compliance checking GuardDuty and Security Hub for continuous monitoring Incident Response Framework:\nPhase 1: Preparation\nDocument response procedures and playbooks Define roles and responsibilities (Incident Commander, Technical Lead, Communications) Establish communication channels (Slack, PagerDuty) Pre-create forensic tools and environments (isolated VPCs, analysis instances) Regular tabletop exercises to test response readiness Phase 2: Detection \u0026amp; Analysis\nAggregate alerts from GuardDuty, Security Hub, CloudWatch Triage based on severity and impact Collect evidence: CloudTrail logs, VPC Flow Logs, memory dumps Determine scope: affected resources, compromised credentials, data exposure Phase 3: Containment\nIsolation: Move compromised instances to forensic VPC for analysis Access revocation: Disable compromised IAM users/roles, rotate credentials Network segmentation: Update security groups to block lateral movement Snapshot preservation: Create EBS snapshots before remediation for forensics Phase 4: Eradication \u0026amp; Recovery\nRemove threats: Terminate malicious instances, delete backdoor accounts Patch vulnerabilities: Update software, fix misconfigurations Rebuild from known-good state: Restore from backups or redeploy via IaC Validation: Scan recovered systems for persistence mechanisms Phase 5: Post-Incident Activities\nDocument lessons learned in incident report Update response procedures based on gaps identified Implement preventive measures to avoid recurrence Share knowledge with broader organization AWS Tools for Incident Response:\nAWS Systems Manager: Session Manager for forensic access without SSH keys CloudFormation StackSets: Deploy forensic infrastructure across accounts Step Functions: Orchestrate multi-step response workflows Lambda: Automated containment actions (isolate instance, revoke credentials) Key Learning Points Defense-in-depth security: Implement multiple layers of security controls across identity, network, data, and application tiers to minimize single points of failure. Least privilege by default: Grant minimal permissions required and expand only when necessary, using permission boundaries to enforce organizational limits. Continuous monitoring is essential: Deploy CloudTrail, GuardDuty, and Security Hub from day one to establish visibility and enable rapid threat detection. Automation over manual processes: Use EventBridge, Lambda, and IaC to automatically remediate security issues and maintain consistent configurations. Encryption everywhere: Protect data at rest with KMS, in transit with TLS, and manage secrets with Secrets Manager for comprehensive data protection. Incident response requires preparation: Develop playbooks, practice scenarios, and pre-deploy tools before incidents occur to minimize response time. Security is a shared responsibility: While AWS secures the infrastructure, customers must properly configure services and implement appropriate controls. Real-World Applications The security principles from this workshop can be directly applied to secure the AI chatbot platform under development.\nIAM Implementation for Chatbot Platform:\nCreate separate IAM roles for different chatbot components (API layer, processing layer, data layer) Use permission boundaries to prevent developers from escalating privileges Implement AWS SSO for team access management instead of individual IAM users Enable MFA for all human access, especially administrative functions Monitoring \u0026amp; Detection:\nEnable CloudTrail in all regions to capture API activity across chatbot infrastructure Deploy GuardDuty to detect compromised credentials or unusual API patterns Create EventBridge rules to alert on sensitive operations (S3 bucket policy changes, security group modifications) Use CloudWatch Logs Insights to analyze chatbot conversation patterns for anomalies Network Security:\nPlace chatbot backend services in private subnets with no direct internet access Use Application Load Balancer in public subnet for user-facing API endpoints Implement Network Firewall to restrict outbound traffic from Lambda functions to approved APIs only Deploy VPC endpoints for AWS service access without traversing internet Data Protection:\nEncrypt conversation history in DynamoDB using KMS customer-managed keys Store user credentials in Secrets Manager with automatic rotation enabled Enable S3 bucket encryption for uploaded files and chatbot training data Enforce TLS for all API communications with ACM-managed certificates Incident Response Preparation:\nCreate incident response runbooks for common scenarios (credential compromise, data breach) Pre-deploy forensic VPC for isolating compromised resources Implement automated containment Lambda functions triggered by GuardDuty findings Conduct quarterly tabletop exercises simulating security incidents Expected Security Outcomes:\nReduced attack surface through least privilege and network isolation Rapid threat detection with automated alerting (minutes vs hours) Automated response to common security events without manual intervention Compliance-ready with audit trails and encryption for sensitive data Faster incident recovery through prepared playbooks and tools Personal Event Highlights This workshop delivered exceptional value through its comprehensive coverage of AWS security services and practical implementation guidance. The session stood out for several reasons:\nEvent Strengths:\nExpert diversity: Bringing together security architects, platform engineers, and student leaders provided multiple perspectives on implementing security Hands-on demonstrations: Live walkthroughs of GuardDuty findings, CloudTrail queries, and incident response procedures made concepts tangible Real-world scenarios: Speakers shared actual security incidents they\u0026rsquo;ve handled, illustrating consequences of misconfigurations Framework alignment: Mapping AWS services to Well-Architected Framework and compliance standards (CIS, PCI DSS) helps with enterprise adoption Community building: AWS Cloud Clubs presentation inspired students and highlighted pathways into cloud security careers Key Takeaway Moments:\nUnderstanding how GuardDuty\u0026rsquo;s machine learning baselines catch subtle anomalies that rule-based systems miss Seeing the power of EventBridge to transform detection into automated response without custom code Learning about permission boundaries as an elegant solution for delegated administration Recognizing the importance of incident response preparation before events occur Networking Impact: Connected with university cloud club leaders and community builders, opening opportunities for:\nCollaboration on security-focused projects and workshops Mentorship relationships with experienced security practitioners Access to security-specific AWS resources and training materials Application to Upcoming Work: The workshop directly influences our chatbot security architecture:\nAdopting GuardDuty for threat detection instead of building custom monitoring Implementing permission boundaries to safely delegate IAM management to team leads Using Secrets Manager with automatic rotation for database and API credentials Establishing incident response procedures before production launch Event Gallery "},{"uri":"https://whopqa.github.io/AWS_Intern/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Problem Statement Manual booking systems or traditional chatbots often encounter difficulties when the customer volume increases, resulting in poor user experience and higher staffing costs for page management. This workshop addresses the issue by building an architecture capable of:\nAutomation: Handling booking, rescheduling, and cancellation 24/7.\nContext understanding: Accurately identifying user intents.\nData retrieval: Responding to complex questions about available time slots and consultant information.\nSolution Architecture The system is designed using an Event-Driven Serverless model combined with VPC-secured networking to ensure security and scalability:\nFrontend Interface: Users interact via Facebook Messenger.\nRequest Handling:\nAmazon API Gateway receives Webhooks from Facebook.\nAWS Lambda (WebhookReceiver) sends messages to an Amazon SQS (FIFO) queue to guarantee ordering and asynchronous processing.\nBrain (Processing Core):\nChatHandler Lambda: Manages conversations and checks sessions from Amazon DynamoDB.\nAuthentication: User verification through Email OTP using Amazon SES.\nAI \u0026amp; Data Layer:\nAmazon Bedrock: Uses Claude 3 Haiku (for Intent Classification), Claude 3.5 Sonnet, and Claude 3 Sonnet (for Text-to-SQL \u0026amp; Extraction).\nAmazon RDS (PostgreSQL): Stores business data (Appointments, Customers, Consultants).\nText2SQL Handler: Converts natural language questions into safe SQL queries to retrieve data.\nAdmin Dashboard \u0026amp; Consultant Portal: A management interface (ReactJS) hosted on Amazon S3 and delivered through CloudFront, allowing administrators to view statistics and manage appointments. Consultant Portal provides the ability to manage booked appointments with customers and view personal schedules.\n(The image illustrates the overall architecture from the Proposal)\nKey Technologies In this workshop, you will work with the following core AWS services:\nAmazon Bedrock: The AI engine, providing Foundation Models (Claude, Titan) for language processing and SQL generation.\nAWS Lambda \u0026amp; API Gateway: Serverless backend without managing servers.\nAmazon RDS (PostgreSQL) + pgvector: Relational database with integrated vector search for RAG.\nAmazon DynamoDB: High-speed storage for session and conversation context.\nAmazon SQS: Ensures reliability and decoupling for message processing.\nAWS CDK (Cloud Development Kit): Deploys the entire infrastructure as code (IaC) using Python.\n"},{"uri":"https://whopqa.github.io/AWS_Intern/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Explore CDK and AWS CLI to prepare for infrastructure development. Refine the overall system architecture and renumber all data flows. Assign tasks within the team and define module ownership. Start implementing the core stacks: Text2SQL and Webhook. Research and integrate Facebook Messenger and Cognito authentication. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study AWS CDK and AWS CLI to prepare for infrastructure deployment 11/10/2025 11/10/2025 AWS Docs 3 - Team meeting \u0026amp; task assignment - Refine and finalize the overall system architecture - Renumber all architecture data flows 11/11/2025 11/11/2025 Internal Architecture Docs 5 - Draft infrastructure structure using the CDK model: + Stack modules (infrastructure deployment) + Service modules (backend logic) + Handler modules (event flow handling) - Research deployment workflow for CDK infrastructure 11/13/2025 11/13/2025 AWS CDK Workshop 6 - Take responsibility for the User flow: handling interactions, consultation, appointment booking, and database writes - Research secure \u0026amp; optimized DB operations - Study methods to optimize booking flow - Start implementing Text2SQL Stack \u0026amp; Webhook Stack - Text2SQL Stack: Implement Lambda to process queries → convert to SQL → execute on PostgreSQL - Webhook Stack: Implement Webhook Verification Lambda, API Gateway GET/POST, IAM Role permissions 11/14/2025 11/14/2025 Internal 6 (continued) - Deploy Messenger App on Facebook Developer - Explore Cognito authentication via Facebook Provider 11/14/2025 11/14/2025 Facebook Developer, AWS Cognito Week 10 Achievements: Gained solid understanding of CDK and AWS CLI fundamentals for infrastructure deployment. Completed refinement of the overall system architecture and renumbered all data flows clearly. Structured the infrastructure into three major module groups: Stack – Deploy CDK resources (Lambda, API Gateway, RDS, IAM, …) Service – Backend logic processing Handler – Event/Lambda flow processing Began implementation of core system components: Text2SQL Stack – Lambda for query → SQL transformation → execute on PostgreSQL Webhook Stack – Lambda for Webhook/Facebook Signature verification, API Gateway GET/POST, IAM Role setup Redesigned the User flow: consultation, booking, safe \u0026amp; optimized DB operations. Researched methods to optimize booking scheduling and avoid conflicts. Deployed Facebook Messenger App and experimented with Cognito authentication via Facebook Provider. "},{"uri":"https://whopqa.github.io/AWS_Intern/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Deploy and complete the Webhook Stack for Facebook Messenger integration. Develop business logic for both Admin and User sides. Optimize system architecture to reduce costs. Research Knowledge Distillation on Amazon Bedrock. Build a secure user authentication system with email-based OTP. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Continue developing Webhook Stack and Backend Webhook - Perform database change operations at row level (by ID) - Learn about Knowledge Distillation on Amazon Bedrock - Attend Workshop event 11/17/2025 11/17/2025 https://cloudjourney.awsstudygroup.com/ 3 - Continue implementing User side - Develop Admin side business logic - Learn about Event mechanism on Lambda 11/18/2025 11/18/2025 https://cloudjourney.awsstudygroup.com/ 4 - Redesign Architecture to reduce costs + Admin retrieves data analytics from Lambda outside VPC + Use DDL/Athena instead of Glue Crawler - Create Jira to manage project progress 11/19/2025 11/19/2025 https://cloudjourney.awsstudygroup.com/ 5 - Deep dive into Cognito security management - Evaluate the feasibility of applying Cognito to the system 11/20/2025 11/20/2025 https://cloudjourney.awsstudygroup.com/ 6 - Successfully deploy Webhook Stack - Create Messenger App on Facebook Developer Dashboard - Configure Page Access Token and Webhook URL - Set up permissions and subscriptions - Debug and fix handler, authentication errors - Verify Messenger connection with Lambda via API Gateway - Switch to email-based OTP authentication (replacing Cognito) - Implement AWS SES Service to send emails - Build user authentication system with session management - Integrate security mechanisms: + Rate limiting (block after 5 incorrect OTP attempts) + Session timeout (reset after 15 minutes) + OTP request throttling (11 seconds between requests) - Add Usage Plan for API Gateway - Verify Webhook and Facebook Signature 11/21/2025 11/23/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Webhook Deployment and Facebook Messenger Integration:\nSuccessfully deployed Webhook Stack connected to Facebook Messenger. Created and configured Messenger App on Facebook Developer Dashboard including: Page Access Token Webhook URL configuration Subscription settings and permissions Understood and implemented important concepts: Page Token: Authentication key to connect Messenger API Verify Token: Key for Messenger to return information to API Gateway PSID (Page-Scoped ID): Unique unchanging ID of each user for each page Successfully connected Messenger with Lambda via API Gateway, verified events on CloudWatch. Architecture and Cost Optimization:\nRedesigned architecture to reduce costs: Admin retrieves data analytics from Lambda outside VPC Used DDL/Athena instead of Glue Crawler to save costs Performed database operations at row level (by ID) to optimize performance. Learned about Events mechanism on Lambda. Research and Learning:\nStudied Knowledge Distillation on Amazon Bedrock. Deep dive into Cognito security management. Attended Workshop event to update new knowledge. Building Secure Authentication System:\nSwitched from Cognito to email-based OTP authentication system due to errors and complexity issues. Successfully implemented AWS SES Service to send OTP emails. Authenticated users and stored sessions in DynamoDB with basic fields. Integrated comprehensive security mechanisms: Rate Limiting: Block account after 5 incorrect OTP attempts in a session Session Management: Reset unverified user status after 15 minutes Request Throttling: OTP requests spaced at least 11 seconds apart API Gateway Protection: Added Usage Plan to control traffic Webhook Security: Verified Facebook Signature to ensure legitimate requests Project Management:\nCreated Jira to systematically manage project progress. Developed business logic for both User and Admin sides. Technical Lessons Learned:\nUnderstood the difference between Page Token and App Secret. Mastered JWT (JSON Web Token) mechanism and its role as security code for applications. Recognized the dynamic nature of information such as Webhook URL, Callback URL, User Pool ID, Client ID when deploying stack - need management measures to avoid changing code multiple times. Successfully debugged errors: Handler not processing events (confused Page Token with App Secret) Cognito login URL not working Stored Session data in DynamoDB with necessary user information fields. "},{"uri":"https://whopqa.github.io/AWS_Intern/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Restructure and optimize Chatbot system architecture. Build efficient session management and caching mechanisms. Develop service files following decoupling principles. Implement intelligent appointment scheduling flow with LLM. Deploy and test appointment scheduling system in production environment. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Design Reset Session mechanism after 15 minutes to save costs - Create confirmation state for appointment information before saving to DB - Restructure code base following decoupling principles: + Session Service: manage session, LLM context, timeout, cache + Bedrock Service: specialized prompts (intent classification, SQL query, response) + Authenticator Service: user authentication + Embed Service: call embedding model and get vector + Indexer Service: create embedding table schema + Messenger Service: send messages to user - Build Handler Files: + Chat Handler: handle conversation logic + Text2SQL Handler: handle query and execute SQL on Facebook - Restructure data flow for Admin 11/24/2025 11/24/2025 https://cloudjourney.awsstudygroup.com/ 3 - Analyze appointment scheduling business questions: + \u0026ldquo;What time slots are available?\u0026rdquo; + \u0026ldquo;I\u0026rsquo;m free now, can you suggest the most suitable time?\u0026rdquo; - Learn main schema in appointment scheduling system - Design optimal appointment scheduling mechanism: + Hold appointments temporarily in cache (2-3 minutes) to avoid conflicts + Handle conflicts when multiple users book simultaneously + Provide confirmation token for first requester - Cache available slots to increase response speed - Check duplicate questions with cache to reuse previous answers - Research Reflection mechanism with LLM when SQL returns incorrect results - Apply Prompt-based Few-shot Learning - Evaluate using SQS - Design Table Cache structure: + Rolling Summary for conversations (token limit) + Cache appointment questions (SQL statement, text, response, query result, prompt, schema, column name) - Setup SQS to avoid Lambda timeout 11/25/2025 11/25/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create function to get conversation context for LLM - Build Context structure for main Model (Sonnet/Haiku): + System Prompt: define bot rules + Structured Summary (from Cache): summarize business state + Recent History: keep last 2-3 conversation turns - Develop Bedrock Service with caching mechanism: + Compare query with previous questions using LLM + Return from cache if relevant + Save metadata for each turn + Keep last 3 turns to save tokens - Complete most Service Files - Build Session Table structure in DynamoDB 11/26/2025 11/26/2025 https://cloudjourney.awsstudygroup.com/ 5 - Add jsonState to appointment field in Session Table - Determine accurate intent classification method (SQL query vs RAG) - Prepare context for bot to understand conversation context - Code Chat Handler to orchestrate conversation logic flow - Handle issue of short questions lacking context - Complete preliminary conversation flow - Ready to test appointment scheduling flow - Notes: + Intent Classify and RAG Service not completed + Session cancel/delete time not set + JSON template for appointment not applied 11/27/2025 11/27/2025 https://cloudjourney.awsstudygroup.com/ 6 - Deploy and test appointment scheduling flow separately - Handle DynamoDB issue not saving float data for vector: + Convert vector to string when writing session + Convert back when searching cache - Change region: + Singapore doesn\u0026rsquo;t have embedding model + Host to Tokyo (trade-off latency) - Enable Claude model in Playground - Develop two specialized Prompts: + Mutation Prompt: constraints for INSERT/UPDATE SQL + Query Prompt: specialized for querying table information - Implement appointment scheduling Logic flow: + Determine intent (query vs appointment) + Divide into 3 sub-flows: Schedule / Update / Cancel + Print available appointment information and index cache + Extract customer information using LLM + Confirm and execute appointment - Identify limitation: Logic may print too many appointments at once 11/28/2025 11/28/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: System Restructuring Following Decoupling Principles:\nSuccessfully built Service-based architecture with independent modules: Session Service: Manage sessions with features: Create new session with predefined fields Update and delete sessions Get session information and context for LLM Check timeout and state Save cache and search using vector search Bedrock Service: Manage specialized prompts: Intent classification SQL queries Generate responses Authenticator Service: User authentication Embed Service: Call embedding model and get vector Indexer Service: Create embedding table schema Messenger Service: Manage sending messages to users Built Handler Files: Chat Handler: Handle conversation logic Text2SQL Handler: Handle query and execute SQL directly on Facebook Restructured data flow for Admin. Session and Caching Mechanism Optimization:\nDesigned Reset Session mechanism after 15 minutes to save costs when appointment information is not confirmed. Created confirmation state to ensure template has complete information before saving to DB. Built Session Table structure in DynamoDB with jsonState field in appointment. Designed intelligent caching mechanism: Rolling Summary: Summarize conversations with token limit via cheaper LLM Query Caching: Store SQL statement, text, response, query result, prompt text, schema_text, column name Recent History: Keep last 3 turns to save tokens Vector Search: Compare query with previous questions using LLM Building Context for LLM:\nDesigned Context structure for main Model (Sonnet/Haiku): System Prompt: Define bot rules Structured Summary (from Cache): Summarize business state (e.g., \u0026ldquo;Customer A, wants appointment on day X, time not confirmed\u0026rdquo;) Recent History: Keep last 2-3 turns verbatim for bot to capture tone and immediate previous questions Prepare context for bot each call to understand conversation context. Developing Intelligent Appointment Scheduling Flow:\nAnalyzed and handled business questions: \u0026ldquo;What time slots are available?\u0026rdquo; \u0026ldquo;I\u0026rsquo;m free now, can you suggest the most suitable time?\u0026rdquo; Designed optimal appointment scheduling mechanism: Temporary hold: Cache appointments for 2-3 minutes to avoid conflicts when multiple users book simultaneously Confirmation token: First requester receives token to continue confirmation Wait notification: Other cases are notified to wait for latest updates Cache available slots: Increase response speed, update when DB changes Check duplicates: Check if user question has same meaning as previous ones to resend old answers Implemented Logic flow with 3 main branches: New appointment: Print available appointment information → User selects → Extract information using LLM → Confirm → Execute Update appointment: Automatically print user\u0026rsquo;s existing appointments → User selects appointment to update → Modify information → Confirm → Execute Cancel appointment: Automatically print user\u0026rsquo;s existing appointments → User selects appointment to cancel → Confirm → Cancel appointment Developing Specialized Prompts:\nMutation Prompt: Contains specialized constraints for INSERT/UPDATE SQL Query Prompt: Specialized for querying table information Researched Reflection mechanism with LLM when SQL returns incorrect results. Applied Prompt-based Few-shot Learning. Deployment and Technical Issue Resolution:\nSuccessfully deployed and tested appointment scheduling flow separately. Handled DynamoDB issue not saving float data: Convert vector to string when writing session Convert back when searching cache Changed region for embedding model: Singapore region doesn\u0026rsquo;t have embedding model Host to Tokyo (trade-off latency) Enabled Claude model in Playground for use. Setup SQS to avoid Lambda timeout. Lessons Learned and Challenges:\nDetermined accurate intent classification method between SQL query and RAG. Handled issue of short questions lacking context (e.g., \u0026ldquo;What time?\u0026rdquo; → Whose time?). Identified limitation: Current logic may print too many appointments at once, not intelligent enough. Incomplete parts: Intent Classify and RAG Service Session cancel/delete time JSON template for appointments "},{"uri":"https://whopqa.github.io/AWS_Intern/5-workshop/5.2-prerequiste/","title":"Prerequisite","tags":[],"description":"","content":"MeetAssist text-to-SQL chatbot using Amazon Bedrock, Amazon DynamoDB, and Amazon RDS To manually create a virtualenv on macOS and Linux:\n$ python3 -m venv .venv After the init process completes and the virtualenv is created, you can use the following step to activate your virtualenv.\n$ source .venv/bin/activate If you are a Windows platform, you would activate the virtualenv like this:\n% .venv\\Scripts\\activate.bat Once the virtualenv is activated, you can install the required dependencies.\n$ pip install -r requirements.txt Prerequisites The following are needed in order to proceed with this post:\nAn AWS account. A Facebook account connected to Facebook Developers for creating the Messenger chatbot. A Facebook Page that will be used to host the chatbot (create a new page or use an existing one). A Git client to clone the source code provided. Docker installed and running on the local host or laptop. Install AWS CDK The AWS Command Line Interface (AWS CLI). The AWS Systems Manager Session Manager plugin. Amazon Bedrock model access enabled for Anthropic Claude 3.5 Sonnet, Claude 3 Sonnet, Claude 3 Haiku and Amazon Titan Embeddings G1 – Text in the ap-northeast-1 Region. Python 3.12 or higher with the pip package manager. Node.js (version 18.x or higher) and npm – required for running the dashboard, installing dependencies, and building workshop assets. "},{"uri":"https://whopqa.github.io/AWS_Intern/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Intelligent Scheduling Chatbot on AWS A Serverless Solution for Automated Appointment Booking \u0026amp; Support 1. Executive Summary The Intelligent Scheduling Chatbot on AWS is a fully serverless solution designed to automate appointment booking and customer support through Facebook Messenger. The system integrates Large Language Models (LLMs) on Amazon Bedrock (Claude 3.5 Sonnet, Claude 3 Haiku, Claude 3 Sonnent) using a Text-to-SQL mechanism to query/execute SQL on a PostgreSQL database. It ensures accurate, context-aware responses and reduces manual scheduling workload by 80–90% while maintaining enterprise-grade security within the Asia Pacific (Tokyo) region.\n2. Problem Statement What’s the Problem? Manual appointment scheduling creates a significant workload for staff, leading to inefficiencies and delayed response times. There is a lack of automated handling for complex queries (e.g., checking specific availability or modifying bookings), and existing support channels often suffer from human error or unavailability outside business hours.\nThe Solution The platform uses Amazon Bedrock to leverage Foundation Models (Claude 3 Family) for intent detection and Text-to-SQL generation. AWS Lambda and Amazon API Gateway handle business logic and Facebook Webhook integration. Amazon RDS (PostgreSQL) stores structured appointment data, while Amazon DynamoDB manages session context. Amazon SES provides secure OTP verification. The system offers an Admin Dashboard hosted on Amazon S3 and CloudFront for consultants to manage schedules efficiently.\nBenefits and Return on Investment The solution reduces manual scheduling effort by 80–90% and ensures an end-to-end response time of ≤ 5 seconds. It provides 99.9% uptime through serverless architecture and ensures strict data security via VPC isolation and Cognito authentication. The estimated monthly cost is $37.24 USD, providing a cost-effective, pay-as-you-go model that scales with traffic, eliminating the need for expensive fixed-server infrastructure.\n3. Solution Architecture The platform employs a Serverless-First and Event-Driven architecture hosted in the ap-northeast-1 region. Incoming messages are buffered via Amazon SQS before being processed by Lambda functions that interact with Amazon Bedrock for AI logic and RDS for data persistence. The architecture is detailed below:\nAWS Services Used Amazon Bedrock: Access to Claude 3 Haiku (Intent), Claude 3 Sonnet (Context), and Claude 3.5 Sonnet (Text2SQL). AWS Lambda: Handles ChatHandler, Text2SQL logic, and Webhook processing. Amazon API Gateway: Secure entry point for Facebook Webhooks and Dashboard APIs. Amazon RDS (PostgreSQL): Stores consultants, customers, and appointment data. Amazon DynamoDB: Manages user sessions and conversation context cache. Amazon SQS (FIFO): Queues incoming messages for asynchronous processing. Amazon SES: Sends emails to users. Amazon Cognito: Manages authentication for the Admin Dashboard. Component Design Frontend Layer: Facebook Messenger via Webhook and an Admin Dashboard (React) hosted on S3/CloudFront. Processing Layer: Lambda functions execute business logic; SQS ensures message ordering; Bedrock handles AI reasoning. Data Layer: RDS for relational data (private subnet); DynamoDB for high-speed session retrieval. Security: VPC Endpoints for private communication; Secrets Manager for credentials; IAM least privilege enforcement. User Management: Amazon Cognito for staff; Facebook Messenger HMAC signature for end-users. 4. Technical Implementation Implementation Phases This project follows an Agile Scrum methodology divided into 4 key phases:\nAWS Learning \u0026amp; Lab Practice: Focus on Serverless fundamentals (Lambda, API Gateway, RDS) and IAM security (September – October). Research \u0026amp; Development: Finalize Text-to-SQL architecture, select Bedrock models, and design the SQL schema (Early November). Core System Development: Build WebhookReceiver, ChatHandler, Text-to-SQL module, and integrate Amazon Bedrock (Mid – Late November). Infrastructure \u0026amp; Dashboard: Deploy VPC/Infrastructure via CDK, develop the Admin Dashboard, and perform end-to-end testing (Late November – December). Technical Requirements\nCore Stack: Python 3.12 (Backend), TypeScript/React (Frontend), AWS CDK v2 (IaC). AI/ML Requirements: Access to Anthropic Claude 3 Haiku, Claude 3.5 Sonnet, Claude 3 Sonnent and Amazon Titan Embeddings G1 via Amazon Bedrock. Database: PostgreSQL with unaccent extension for Vietnamese language support; CSV data imports for initialization. External Dependencies: Meta (Facebook) Developer Account for Graph API v18.0; Verified Amazon SES identity. 5. Timeline \u0026amp; Milestones Project Timeline\nSeptember – October: AWS Learning \u0026amp; Hands-On Labs. Early November: Architecture Design \u0026amp; Technology Selection. Mid November: Core Backend Development (ChatHandler, SQL Module). Late November: Infrastructure Deployment \u0026amp; Admin Dashboard Frontend. December: Integration Testing, Optimization, and Final Presentation. 6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator.\nInfrastructure Costs AWS Services: Amazon VPC: $20.45/month (2 Interface Endpoints for security). Amazon Bedrock: $9.08/month (Tokens for Claude 3 Haiku/Sonnet and Claude 3.5 Sonnent models). Amazon RDS: $5.83/month (db.t3.micro, 20GB storage). Amazon Cognito: $0.50/month (10 MAU). Amazon SQS: $0.50/month (1M requests). AWS Secrets Manager: $0.40/month. Amazon DynamoDB: $0.28/month (On-demand). Amazon SES \u0026amp; S3: \u0026lt;$0.10/month. API Gateway, Lambda, EventBridge: $0.00 (Covered by Free Tier). Total: $37.24/month, ~$446.88/12 months.\n7. Risk Assessment Risk Matrix LLM Accuracy (Hallucinations): Medium impact, medium probability. Cost Variance: Low impact, medium probability (Pay-as-you-go fluctuations). Data Privacy: High impact, low probability. SQL Generation Failure: High impact, medium probability. Insufficient Context (Contextual Awareness): High impact, low probability. OTP Brute-force Attack: High impact, low probability. Mitigation Strategies Accuracy: Strict prompt engineering and database user permission guardrails (SELECT/INSERT only). Cost: VPC Endpoint optimization (remove when idle) and Free Tier usage. Privacy: Database in Private Isolated Subnets; PII compliance governance. SQL Generation: Prompt validation is strictly implemented to handle enum constraints, verify column/table names, and correctly format WHERE clauses (including LIKE operators) for Vietnamese names/text. Context: System prompts include mandatory function calls to retrieve additional session context, ensuring the LLM fully understands the user\u0026rsquo;s message history before responding. OTP Security: Rate limiting is enforced (max 5 attempts/session, 15s cooldown), with auto-blocking for 3600s after failures. OTPs expire in 5 minutes, and verification uses HMAC timing-attack safe methods. 8. Expected Outcomes Technical Improvements: Automated, context-aware scheduling via SQL generation. End-to-end response latency ≤ 5 seconds. Secure, VPC-isolated infrastructure with 99.9% uptime.\nLong-term Value Scalable foundation for omni-channel expansion (Web/Mobile). Data collection for future advanced analytics (AWS Glue/Athena). Operational excellence through reduced manual administrative work.\n"},{"uri":"https://whopqa.github.io/AWS_Intern/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Improving accuracy for your cloud budgeting with new features in AWS Budgets This blog introduces new features in AWS Budgets that help you track and manage your AWS spending more accurately. Improvements include support for additional cost metrics (net unblended, net amortized), the ability to exclude specific dimensions when creating budgets, detailed fee filtering options, and enhanced API support for filter expressions compatible with AWS Cost Explorer. The article guides you on how to use these features in the console and via CLI, and provides best practices for effective cloud financial management.\nBlog 2 - Increase engagement with localized content using Amazon Bedrock Flows This blog demonstrates how to use Amazon Bedrock Flows, Prompt Management, and foundation AI models to automate the localization workflow for new markets. It shows how to build a no-code workflow to localize articles, evaluate localization quality, and assess metrics such as factual accuracy, length, dialect, and meaning. Editors can easily review and publish content tailored for each market. The article also covers setup, flow testing, and notes on service pricing.\nBlog 3 - Innovating at Speed: BMW\u0026rsquo;s Generative AI Solution for Cloud Incident Analysis This blog explains how BMW leverages Amazon Bedrock Agents and the ReAct framework to automate root cause analysis (RCA) for cloud-based digital services. It details how integrated AI tools (CloudWatch, CloudTrail, Lambda) analyze architecture, logs, metrics, and infrastructure to dramatically reduce incident diagnosis time from hours to minutes. A real-world case study demonstrates how AI empowers engineers—including juniors—to quickly identify and resolve issues, improving service reliability for millions of BMW customers worldwide.\n"},{"uri":"https://whopqa.github.io/AWS_Intern/5-workshop/5.3-enablebedrockmodels/","title":"Enable Bedrock Models","tags":[],"description":"","content":"Enabling Bedrock Models Before deploying the solution, you need to enable the required Amazon Bedrock models in your AWS account.\nSteps to Enable Models Search for Amazon Bedrock in AWS Console 2. Access Model catalog from the left navigation menu\n3. Choose the corresponding model name:\nAnthropic Claude 3.5 Sonnet Anthropic Claude 3 Sonnet Anthropic Claude 3 Haiku Amazon Titan Embeddings G1 – Text 4. Select \u0026ldquo;Open in playground\u0026rdquo; and send a test message to enable each model\nMake sure you enable all four models in the ap-northeast-1 (Tokyo) region as the solution is deployed in this region.\n"},{"uri":"https://whopqa.github.io/AWS_Intern/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in 3 events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS\nDate \u0026amp; Time: 09:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AWS Cloud Mastery Series #2 – DevOps on AWS\nDate \u0026amp; Time: 09:00, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS Cloud Mastery Series #3 – Security Pillar Workshop\nDate \u0026amp; Time: 09:00, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://whopqa.github.io/AWS_Intern/5-workshop/5.4-configureawscli/","title":"Configure AWS CLI","tags":[],"description":"","content":"Configuration AWS CLI To deploy and manage the solution, you need to configure the AWS Command Line Interface (AWS CLI) with your credentials.\nSteps Type aws configure in your terminal. Make sure you have created CLI secret access key for your account (recommend using an IAM account with admin privileges)\nComplete the configuration form:\nAWS Access Key ID: Your access key AWS Secret Access Key: Your secret key Default region name: ap-northeast-1 Default output format: json Creating IAM Access Keys To create IAM access keys, follow these steps:\nGo to AWS Console → IAM → Users → Your User Navigate to Security Credentials tab Click Create Access Key Download or save your credentials securely Never share your AWS access keys or commit them to version control. Store them securely and rotate them regularly.\nVerification After configuration, verify your setup by running:\naws sts get-caller-identity You should see your account ID, user ARN, and user ID in the response.\n"},{"uri":"https://whopqa.github.io/AWS_Intern/5-workshop/5.5-preparedata/","title":"Prepare Data","tags":[],"description":"","content":"Download the Dataset and Upload to Amazon S3 Step 1: Download the Dataset Navigate to the MeetAssistData Download the data to your local machine Unzip the file, which will expand into a folder called DATA IMPORTANT - Do NOT open CSV files with Excel:\nMicrosoft Excel can automatically convert data formats, which may corrupt the data (e.g., phone numbers, dates, IDs may be altered) Recommended approach: Upload the data to S3 first (steps below), then use VS Code to view/validate the CSV files instead of Excel If you must view locally: Use VS Code only - it won\u0026rsquo;t modify your data like Excel does Keep original CSV files intact before uploading to S3 Step 2: Create S3 Bucket Run this CLI script to create the Amazon S3 bucket. Replace \u0026lt;account-id\u0026gt; with your AWS account ID:\naws s3 mb s3://meetassist-data-\u0026lt;account-id\u0026gt;-ap-northeast-1 --region ap-northeast-1 To find your AWS Account ID, run: aws sts get-caller-identity --query Account --output text\nStep 3: Upload Data to S3 Go to the AWS S3 Console Navigate to the bucket you just created: meetassist-data-\u0026lt;account-id\u0026gt;-ap-northeast-1 Create a folder named data Upload all CSV files from the unzipped DATA folder into the data folder in S3 Verification After uploading, verify that all CSV files are present in your S3 bucket under the data/ prefix:\naws s3 ls s3://meetassist-data-\u0026lt;account-id\u0026gt;-ap-northeast-1/data/ You should see all the CSV files listed in the output.\n"},{"uri":"https://whopqa.github.io/AWS_Intern/5-workshop/","title":"Workshop","tags":[],"description":"","content":"AI-Powered Schedule Booking Chatbot on AWS Overview In this workshop, we will build MeetAssist — an intelligent chatbot system running on the AWS Serverless platform. This solution applies Generative AI (LLM) to automate the process of appointment scheduling and customer information lookup through a Facebook Messenger interface.\nInstead of responding based on a fixed rule-based script, the system uses a Text-to-SQL model to understand natural language, query real-time data from the database, and provide flexible responses to users.\nContent Workshop overview Prerequisites Enable Bedrock Models Configure AWS CLI Prepare Data Deploy Solution Setup Messenger Bot Setup Admin Dashboard Using the Chatbot Using the Admin Dashboard Using the Consultant Portal Clean Up Resources "},{"uri":"https://whopqa.github.io/AWS_Intern/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at First Cloud Journey (FCJ) from September 8, 2025 to December 7, 2025, I had the opportunity to learn, practice, and apply AWS Cloud knowledge in a real-world working environment.\nI participated in designing and developing an AI Chatbot system using Amazon Bedrock, Lambda, RDS PostgreSQL, DynamoDB, and other AWS services, through which I improved my skills in full-stack development with AWS CDK, prompt engineering, AI system architecture, data processing, and cloud cost optimization.\nAI and Machine Learning Knowledge Applied:\nAmazon Bedrock \u0026amp; Large Language Models (LLM): Used Claude 3 Haiku and Claude 3.5 Sonnet to build intelligent conversational chatbot, processing Vietnamese natural language. Prompt Engineering: Designed and optimized specialized prompts for intent classification, information extraction, SQL query generation, and context-aware responses. Applied Few-shot Learning and Reflection technique to improve accuracy. Vector Search \u0026amp; Embeddings: Used embedding models to convert text to vectors, built schema search mechanism and question caching based on cosine similarity. Retrieval-Augmented Generation (RAG): Researched and applied RAG architecture combining Amazon Kendra, LangChain agent, and knowledge base for chatbot to answer based on enterprise data sources. Text-to-SQL Generation: Built pipeline to convert natural language questions to SQL queries, combining context awareness and schema annotation to increase accuracy. Session Management \u0026amp; Context Window: Designed conversation context management mechanism with Rolling Summary, token limitation, and caching to optimize LLM API costs. Model Selection \u0026amp; Cross-Region Inference: Compared and selected appropriate models (Haiku vs Sonnet), balancing quality and latency when using cross-region. Important AWS Foundation Knowledge:\nCompute \u0026amp; Serverless: Lambda, API Gateway, EventBridge, SQS - building event-driven architecture and handling throttling/timeout. Database: RDS PostgreSQL (ACID, transaction, unaccent extension), DynamoDB (NoSQL, session storage), Vector Search. Storage: S3 (static website, lifecycle, versioning), Storage Gateway, FSx. Networking: VPC, Subnet, Security Group, NAT Gateway, Transit Gateway, Site-to-Site VPN, Route 53 Resolver. Security \u0026amp; IAM: IAM User/Role/Policy, Permission Boundary, KMS encryption, Security Hub, Cognito, Organizations, Identity Center. Infrastructure as Code: AWS CDK (TypeScript) to deploy entire stack: Text2SQL, Webhook, Database, Networking following decoupling model. Monitoring \u0026amp; Cost Optimization: CloudWatch, Budget, Cost Explorer, auto start/stop EC2, reducing unnecessary VPC Endpoints. Data Analytics: Data Lake, Athena, Glue, QuickSight, Redshift, ElastiCache. Valuable Experience and Lessons:\nThe FCJ internship program not only equipped me with technical knowledge but also provided invaluable real-world experiences:\nProfessional Work Environment: Experienced technology enterprise standard work culture, understood product development process from research, design, development to deployment and monitoring. Learned to work with project management tools, version control (Git), documentation (Notion), and best practices in team collaboration.\nReal-World Perspective on Cloud Architecture: Gained clearer understanding of how real enterprise projects are deployed on AWS - from selecting appropriate services, designing scalable and cost-effective architecture, handling production issues like throttling/timeout/security, to optimizing performance and monitoring systems. Understood the difference between \u0026ldquo;doing labs\u0026rdquo; and \u0026ldquo;doing real projects\u0026rdquo; - where multiple factors must be considered such as business logic, user experience, cost, security, and maintainability.\nLearning from Community: Learned from talented peers in the program - each person had their own strengths in frontend, backend, DevOps, or AI/ML. Code review, architecture discussions, and pair programming helped broaden thinking and learn different approaches to the same problem.\nMentorship from Experienced Professionals: Received dedicated guidance from mentors on technical skills, design thinking, and problem-solving approach. Questions like \u0026ldquo;why choose this solution?\u0026rdquo;, \u0026ldquo;is there a better way?\u0026rdquo;, \u0026ldquo;what if we scale to 1000x users?\u0026rdquo; helped me think deeper and avoid common mistakes. Learned to read documentation effectively, debug systematically, and most importantly the \u0026ldquo;always learning\u0026rdquo; mindset in technology.\nSoft Skills: Improved ability to present ideas, write proposals, conduct demos, and receive constructive feedback. Learned time management when working on multiple tasks in parallel and handling pressure when encountering critical bugs before deadlines.\nIn terms of work ethic, I always strived to complete tasks well, proactively researched new technologies, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ✅ ☐ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Areas for Improvement Discipline and Time Management: Strengthen adherence to regulations, work schedules, and project deadlines. Problem-Solving Mindset: Improve ability to analyze complex problems, break down large tasks into smaller steps, and deliver optimal solutions quickly. Communication Skills: Learn to present ideas more clearly, proactively report progress, and handle difficult situations in teamwork professionally. Important Lessons from AI Chatbot Project Model temperature significantly impacts accuracy of information extraction - requires careful adjustment for each use case. Combining multiple prompts into one reduces API calls, increases consistency, and saves costs. Vector search requires careful fine-tuning with annotations and similarity thresholds to achieve optimal results. Cross-region inference is a trade-off between model quality and latency - requires balancing based on actual requirements. SQS is an effective solution for throttling and timeout issues in serverless architecture. Vietnamese text processing requires unaccent extension in PostgreSQL for accurate accent-insensitive search. Business logic constraints are crucial to prevent invalid data and ensure data integrity. Decoupling architecture makes code easier to maintain, test, and scale - applying Service pattern to separate Session, Bedrock, Database, Messenger. Context window management determines cost and performance - Rolling Summary and cache strategy are key. Prompt engineering is an art requiring extensive experimentation and deep understanding of business logic - Few-shot Learning and Reflection technique significantly improve accuracy. "},{"uri":"https://whopqa.github.io/AWS_Intern/5-workshop/5.6-deploysolution/","title":"Deploy the Solution","tags":[],"description":"","content":"Deploy the Solution In this section, we will clone the MeetAssist repository and deploy the complete infrastructure using AWS CDK.\nStep 1: Clone the Repository Clone the repository from GitHub:\ngit clone https://github.com/AWS-Vinhomes-Chatbot/MeetAssist cd MeetAssist Step 2: Build the Dashboard Before deploying the CDK application, we need to build the frontend dashboard.\nNavigate to frontend folder cd frontend Install dependencies Run the following command to install all necessary libraries:\nnpm i Build the Dashboard After the installation is complete, run the build command:\nnpm run build Once the process completes, a dist directory will be created, containing the index.html file and the assets folder.\nReturn to project root Use this command to return to the project\u0026rsquo;s root folder:\ncd .. Step 3: Deploy the CDK Application Deploy the CDK application. It will take about 20-30 minutes to deploy all of the resources.\ncdk bootstrap aws://{{account_id}}/ap-northeast-1 cdk deploy --all If you receive an error at this step, please ensure Docker is running on the local host or laptop.\nReplace {{account_id}} with your actual AWS Account ID. You can find it by running:\naws sts get-caller-identity --query Account --output text Step 4: Initialize the Database with Embeddings After the CDK deployment completes, you must run the DataIndexer Lambda function to populate the embeddings table with database schema information.\nUse the following AWS CLI command:\naws lambda invoke \\ --function-name DataIndexerStack-DataIndexerFunction \\ --invocation-type Event \\ response.json \\ --region ap-northeast-1 The DataIndexer function generates vector embeddings of your database schema, which enables the chatbot to understand and query your data using natural language.\nStep 5: Verify Deployment After completing all the steps above, your environment is fully deployed and initialized.\nYou can verify the deployment by checking:\nCloudFormation Stacks: All stacks should show CREATE_COMPLETE status outputs.json file: Contains important URLs and IDs for your deployment S3 Buckets: Dashboard assets should be uploaded Lambda Functions: All functions should be created and ready Next Steps Now you can proceed to:\nConfigure the Facebook Messenger integration (Section 5.7) Set up the Admin Dashboard (Section 5.8) Start using the MeetAssist chatbot (Section 5.9) "},{"uri":"https://whopqa.github.io/AWS_Intern/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nFCJ creates an incredibly friendly and open working environment. Especially, the mentors and team members are always enthusiastic and ready to help whenever I encounter difficulties at work. The office is neatly arranged and equipped with modern support facilities, helping me work comfortably and professionally.\n2. Support from Mentor / Team Admin\nThe mentors provide extremely enthusiastic and detailed guidance, always patiently explaining clearly whenever I don\u0026rsquo;t understand an issue. Particularly, during the process of perfecting the project architecture, they helped me improve the design and identify critical core issues that I hadn\u0026rsquo;t noticed. The admin team is also very dedicated in supporting administrative procedures, providing documents, and creating all favorable conditions for work. I especially appreciate the mentors\u0026rsquo; teaching method - instead of providing ready answers, they encourage me to think independently, experiment, and solve problems myself, which has helped me learn much more deeply.\n3. Relevance of Work to Academic Major Although the main knowledge in this period is about cloud, which is quite unrelated to my current major as an AI Engineer, this is a rare opportunity to expand into new areas related to cloud that I have never been exposed to before. Most importantly, I gained a real-world perspective on technical aspects as well as other aspects of how an AI product operates in actual enterprises.\n4. Learning \u0026amp; Skill Development Opportunities\nThe FCJ internship program provided me with countless opportunities for comprehensive skill development. In terms of technical skills, I learned how to use AWS CDK to deploy Infrastructure as Code, work with AI/ML services like Bedrock, design complex system architectures, and apply prompt engineering in practice. Regarding soft skills, I was trained in using project management tools (Git, Notion, draw.io, Jira, etc.), teamwork skills, and professional communication in a corporate environment. Particularly, the mentors also shared many valuable experiences such as workplace etiquette, effective work styles, interview skills, and foundational knowledge needed to become a professional engineer. These lessons have given me a clearer vision of my career path and how to develop myself in the future.\n5. Company Culture \u0026amp; Team Spirit\nFCJ has a very positive and professional work culture. Everyone always respects and supports each other, working seriously but still maintaining a friendly and pleasant atmosphere. Especially, when there are urgent projects or important deadlines, the whole team always stands together, supporting each other regardless of position or rank. This makes me - even though I\u0026rsquo;m just an intern - still feel valued and an important part of the collective. The company also regularly organizes knowledge-sharing workshops by Mr. Hung and other mentors, which has greatly boosted my spirit of contribution and confidence.\n6. Internship Policies / Benefits\nFCJ has very good policies supporting interns, especially in creating flexible time arrangements when needed to help me balance study and work. Additionally, I was able to participate in events, workshops, and training sessions on cloud computing organized by the company - these are all beneficial activities that help me enhance my professional knowledge and expand my network.\nAdvice for Fellow Students If you have the opportunity to intern at FCJ, I highly recommend you participate. This is truly a great opportunity for you to learn and familiarize yourself with cloud computing - a new, practical technology that is receiving much attention in the industry. The program not only equips solid technical knowledge but also helps you develop soft skills, work in a professional environment, and more importantly, turns your ideas into reality more easily through AWS services.\nSuggestions \u0026amp; Expectations Improvement Suggestions:\nI hope the company can organize more meetings and sharing sessions specifically for FPT students to create stronger bonds and learn more from each other\u0026rsquo;s experiences. Could add more training sessions on soft skills such as presentation, communication, or career orientation. Personal Wishes:\nI really hope to have the opportunity to continue working with FCJ in the future, whether on larger projects or as a mentorship program for the next generation of interns. "},{"uri":"https://whopqa.github.io/AWS_Intern/5-workshop/5.7-setupmessengerbot/","title":"Setup Messenger Bot","tags":[],"description":"","content":"Setting Up the Messenger Chatbot The MeetAssist chatbot is integrated with Facebook Messenger, allowing users to interact naturally to book, update, or cancel appointments.\nStep 1: Create a Facebook App Go to Facebook Developers Click \u0026ldquo;My Apps\u0026rdquo; Select \u0026ldquo;Create an app\u0026rdquo; Fill in your app name (e.g., demo) and App Contact Email → Click \u0026ldquo;Next\u0026rdquo; Select \u0026ldquo;Messaging businesses\u0026rdquo; as your use case → Click \u0026ldquo;Receive\u0026rdquo; Select your \u0026ldquo;Business\u0026rdquo; profile or choose none if you just want to test the app → Click \u0026ldquo;Receive\u0026rdquo; Click \u0026ldquo;Go to Dashboard\u0026rdquo; Step 2: Configure App Settings In your app dashboard, click \u0026ldquo;App Settings\u0026rdquo; → \u0026ldquo;Basic info\u0026rdquo; Copy and save your App ID and App Secret Paste the Privacy Policy URL: https://www.freeprivacypolicy.com/live/e7193dae-4bba-4482-876e-7b76d83a0676 Select \u0026ldquo;Messenger for Business\u0026rdquo; as the app category → Click \u0026ldquo;Save Changes\u0026rdquo; Step 3: Store Facebook Credentials in AWS Run the following AWS CLI commands to securely store your Facebook credentials:\nCreate SSM Parameters: # Facebook App ID aws ssm put-parameter ` --name \u0026#34;/meetassist/facebook/app_id\u0026#34; ` --value \u0026#34;YOUR_FACEBOOK_APP_ID\u0026#34; ` --type \u0026#34;String\u0026#34; ` --description \u0026#34;Facebook App ID for MeetAssist\u0026#34; ` --region ap-northeast-1 # Facebook App Secret aws ssm put-parameter ` --name \u0026#34;/meetassist/facebook/app_secret\u0026#34; ` --value \u0026#34;YOUR_FACEBOOK_APP_SECRET\u0026#34; ` --type \u0026#34;String\u0026#34; ` --description \u0026#34;Facebook App Secret for MeetAssist\u0026#34; ` --region ap-northeast-1 Create Secrets Manager Secrets: # Facebook Page Access Token (get this from step 4 below) aws secretsmanager create-secret ` --name \u0026#34;meetassist/facebook/page_token\u0026#34; ` --description \u0026#34;Facebook Page Access Token for MeetAssist\u0026#34; ` --secret-string \u0026#34;YOUR_FACEBOOK_PAGE_ACCESS_TOKEN\u0026#34; ` --region ap-northeast-1 # Facebook Verify Token (create a random string, e.g., \u0026#34;my_secure_token_12345\u0026#34;) aws secretsmanager create-secret ` --name \u0026#34;/meetassist/facebook/verify_token\u0026#34; ` --description \u0026#34;Facebook Webhook Verify Token for MeetAssist\u0026#34; ` --secret-string \u0026#34;YOUR_CUSTOM_VERIFY_TOKEN_123456\u0026#34; ` --region ap-northeast-1 Replace YOUR_FACEBOOK_APP_ID, YOUR_FACEBOOK_APP_SECRET, YOUR_FACEBOOK_PAGE_ACCESS_TOKEN, and YOUR_CUSTOM_VERIFY_TOKEN_123456 with your actual values.\nStep 4: Connect Facebook Page and Get Page Access Token In your app dashboard, click \u0026ldquo;Use Cases\u0026rdquo; → \u0026ldquo;Customize\u0026rdquo; Go to \u0026ldquo;Install the Messenger API\u0026rdquo;, click \u0026ldquo;Connect\u0026rdquo; to link your Facebook Page to the app Select \u0026ldquo;Create\u0026rdquo; to copy the generated Page Access Token Use this token in the aws secretsmanager create-secret command above Step 5: Configure Webhooks API Get your Webhook URL from the outputs.json file (generated after CDK deployment) In your app dashboard, go to \u0026ldquo;Messenger API Settings\u0026rdquo; Go to the \u0026ldquo;Configure Webhooks\u0026rdquo; section Enter the following information: Callback URL: https://\u0026lt;your-api-gateway-url\u0026gt;/webhook (from outputs.json) Verify Token: The same random string you used in step 3 (e.g., YOUR_CUSTOM_VERIFY_TOKEN_123456) Click \u0026ldquo;Verify and Save\u0026rdquo; Subscribe to the following webhook fields: messages messaging_postbacks messaging_account_linking Step 6: Subscribe Page to Your App (This step is automatically handled when you connect the page in Step 4, so you can skip this if already done)\nStep 7: Configure Messenger Profile (Get Started Button \u0026amp; Greeting) After deployment, the chatbot automatically configures:\nGet Started Button: Users click this to begin conversation Greeting Text: Welcome message shown before user starts chatting To manually update if needed:\n# Set Get Started Button curl -X POST \u0026#34;https://graph.facebook.com/v18.0/me/messenger_profile?access_token=\u0026lt;PAGE_ACCESS_TOKEN\u0026gt;\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;get_started\u0026#34;: {\u0026#34;payload\u0026#34;: \u0026#34;GET_STARTED\u0026#34;} }\u0026#39; # Set Greeting Text curl -X POST \u0026#34;https://graph.facebook.com/v18.0/me/messenger_profile?access_token=\u0026lt;PAGE_ACCESS_TOKEN\u0026gt;\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;greeting\u0026#34;: [ { \u0026#34;locale\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Xin chào! Mình là MeetAssist, trợ lý đặt lịch hẹn tư vấn hướng nghiệp. Hãy nhấn \\\u0026#34;Bắt đầu\\\u0026#34; để sử dụng dịch vụ! 👋\u0026#34; } ] }\u0026#39; Step 8: App Mode and Permissions Development Mode: Your app is currently in development mode. Only you (the app developer) and testers you add can interact with the bot.\nPublic Access: To allow other users to use your bot, go to your app dashboard and switch the app to \u0026ldquo;Live Mode\u0026rdquo; by selecting \u0026ldquo;Post\u0026rdquo;\nApp Review: For full features and permissions, you must complete Facebook\u0026rsquo;s App Review process. For testing purposes only, App Review is not required.\nVerification Test your bot by:\nOpen your Facebook Page in Messenger Click the \u0026ldquo;Get Started\u0026rdquo; button Verify you receive the greeting message Try sending a test message to confirm the webhook is working If everything is configured correctly, the bot should respond to your messages!\n"},{"uri":"https://whopqa.github.io/AWS_Intern/5-workshop/5.8-setupadmindashboard/","title":"Setup Admin Dashboard","tags":[],"description":"","content":"Setup Admin Dashboard The Admin Dashboard requires an administrator account created in Amazon Cognito. Follow these steps to access the dashboard for the first time.\nStep 1: Create an Admin User Run the following AWS CLI command to create an admin account:\naws cognito-idp admin-create-user --user-pool-id \u0026lt;your-user-pool-id\u0026gt; --username \u0026lt;your-email\u0026gt; --user-attributes Name=email,Value=\u0026lt;your-email\u0026gt; Name=email_verified,Value=true --temporary-password \u0026#34;\u0026lt;your-temporary-password\u0026gt;\u0026#34; --region ap-northeast-1 Both the User Pool ID and the Admin Dashboard URL can be found in the outputs.json file generated after CDK deployment.\nStep 2: First Login Open the Admin Dashboard URL from the outputs.json file Log in with your email and temporary password Cognito will prompt you to set a new permanent password After updating your password, you will be redirected to the Admin Dashboard Step 3: Verify Access After logging in, you should see the dashboard homepage with:\nSystem statistics (total customers, consultants, appointments) Appointments breakdown by status Recent appointments list You can create additional admin users by repeating Step 1 with different email addresses.\nTroubleshooting Issue: Cannot login\nVerify the User Pool ID is correct Check if the user was created successfully in Cognito Console Ensure you\u0026rsquo;re using the correct temporary password Now you\u0026rsquo;re ready to use the Admin Dashboard to manage consultants and appointments!\n"},{"uri":"https://whopqa.github.io/AWS_Intern/5-workshop/5.9-usingchatbot/","title":"Using the Chatbot","tags":[],"description":"","content":"Using the Chatbot The MeetAssist chatbot can be accessed via Facebook Messenger and provides a conversational interface for booking appointments with consultants.\nGetting Started Open Facebook Messenger and search for your connected page Click the Get Started button The chatbot will greet you and request authentication Authentication Flow Before using the chatbot, you need to authenticate:\nEmail Verification: Enter your registered email address OTP Request: The bot will send a One-Time Password to your email Enter OTP: Enter the 6-digit code from your email (check spam folder) Active Session: Your session is valid for 24 hours If your AWS SES is in sandbox mode, you can only send emails to verified addresses. For production use, request SES production access.\nBooking an Appointment The chatbot uses natural language understanding capabilities powered by Amazon Bedrock. You can book appointments using conversational Vietnamese:\nExample conversations:\n\u0026ldquo;Tôi muốn đặt lịch với tư vấn viên Nguyễn Văn A vào thứ 2 tuần sau lúc 10 giờ sáng\u0026rdquo; \u0026ldquo;Book appointment with consultant John next Monday at 2pm\u0026rdquo; \u0026ldquo;Đặt lịch gặp chuyên gia Trần Thị B ngày mai buổi chiều\u0026rdquo; The chatbot will:\nExtract appointment information (consultant, date, time) Check the consultant\u0026rsquo;s available schedule Confirm the booking with you Create the appointment in the system Updating an Appointment To edit an existing appointment:\n\u0026ldquo;Tôi muốn đổi lịch hẹn sang thứ 4\u0026rdquo; \u0026ldquo;Change my appointment to 3pm\u0026rdquo; \u0026ldquo;Reschedule my meeting with consultant A\u0026rdquo; The chatbot will display your current appointments and guide you through the update process.\nCanceling an Appointment To cancel an appointment:\n\u0026ldquo;Hủy lịch hẹn của tôi\u0026rdquo; \u0026ldquo;Cancel my appointment\u0026rdquo; \u0026ldquo;I want to cancel my meeting\u0026rdquo; The bot will list your active appointments and request confirmation before canceling.\nGeneral Queries Ask the chatbot about:\nConsultants:\n\u0026ldquo;Có những tư vấn viên nào?\u0026rdquo; \u0026ldquo;Who are the available consultants?\u0026rdquo; \u0026ldquo;Tell me about consultant Nguyễn Văn A\u0026rdquo; Available Schedule:\n\u0026ldquo;Tư vấn viên A có lịch trống khi nào?\u0026rdquo; \u0026ldquo;Show me available slots for next week\u0026rdquo; \u0026ldquo;When is consultant B free?\u0026rdquo; Your Appointments:\n\u0026ldquo;Lịch hẹn của tôi\u0026rdquo; \u0026ldquo;My appointments\u0026rdquo; \u0026ldquo;Show my upcoming meetings\u0026rdquo; Aborting Actions If you want to cancel the current conversation flow:\nType \u0026ldquo;abort\u0026rdquo; or \u0026ldquo;hủy\u0026rdquo; at any time The bot will reset and you can start a new request Session Management Sessions expire after 24 hours You will need to re-authenticate with email + OTP Your conversation history is maintained throughout the session Troubleshooting Issue: Bot not responding\nCheck if the Facebook webhook is configured correctly Verify the API Gateway URL in webhook settings Check Lambda logs in CloudWatch Issue: Not receiving OTP\nVerify your email is correct Check spam/junk folder If in SES sandbox mode, ensure the email is verified in SES Console Issue: Appointment booking fails\nEnsure the consultant exists in the database Check if the requested time slot is available Verify the date format is understood by the bot Issue: Bedrock model throttling\nIf you see slow responses, you may be hitting Bedrock rate limits Consider requesting quota increases in Service Quotas console The chatbot learns from context. If it doesn\u0026rsquo;t understand, try rephrasing your request with more specific details like exact dates and times.\nNow you can start interacting with the MeetAssist chatbot to manage your appointments!\n"},{"uri":"https://whopqa.github.io/AWS_Intern/5-workshop/5.10-usingadmindashboard/","title":"Using the Admin Dashboard","tags":[],"description":"","content":"Using the Admin Dashboard The Admin Dashboard provides a comprehensive interface for managing consultants, schedules, and appointments.\nOverview Page After logging in, the dashboard homepage displays:\nSystem Statistics:\nTotal number of customers Total number of consultants Total number of appointments Appointments Breakdown:\nVisual chart showing appointments by status: Pending Confirmed Completed Cancelled Recent Appointments:\nList of the most recent appointments Quick view of customer, consultant, date, and status Consultants Management Navigate to the Consultants section to manage your consultant team.\nView Consultants See all consultants in a table view Columns: Name, Email, Specialization, Account Status Add a New Consultant Click the Add Consultant button Fill in the consultant details: Full Name Email Phone Number Specialization Qualifications Click Create Creating a consultant will automatically create a corresponding Cognito user account with temporary credentials sent to their email.\nEdit Consultant Information Click the Edit button next to a consultant Update the desired fields Click Save Changes Delete a Consultant Click the Delete button next to a consultant Confirm the deletion The consultant\u0026rsquo;s Cognito account will also be removed Reset Consultant Password Click the Reset Password button A new temporary password will be generated The consultant will receive the password via email Appointments Management Navigate to the Appointments section for comprehensive appointment oversight.\nView Appointments Table view of all appointments Filter by: Status (Pending, Confirmed, Completed, Cancelled) Date range Consultant Customer Create Manual Appointment Click Create Appointment Select customer (or create new) Select consultant Choose date and time Add notes (optional) Click Book Appointment Update Appointment Click Edit on an appointment Modify details (time, consultant, notes) Click Update Change Appointment Status Manually update appointment status:\nPending → Confirmed: Approve a booking Confirmed → Completed: Mark as finished Any → Cancelled: Cancel the appointment You now have full control over the MeetAssist system through the Admin Dashboard!\n"},{"uri":"https://whopqa.github.io/AWS_Intern/5-workshop/5.11-usingconsultantportal/","title":"Using the Consultant Portal","tags":[],"description":"","content":"Using the Consultant Portal The Consultant Portal is a dedicated interface for consultants to manage their appointments and view their schedules.\nAccessing the Consultant Portal Open the Consultant Portal URL from the outputs.json file or URL from FrontendStack Outputs in the AWS CloudFormation console. Log in with your consultant email and password First-time users will need to change their temporary password Consultant accounts are created by administrators through the Admin Dashboard. You will receive your login credentials via email.\nMy Appointments Page The My Appointments page is your central hub for managing all your scheduled meetings.\nView Appointments See all your appointments in a table view with:\nCustomer name and contact Appointment date and time Status (Pending, Confirmed, Completed, Cancelled) Filter Appointments Use the filter options to narrow down your view:\nBy Status: Show only pending, confirmed, or completed appointments By Date Range: View appointments for specific time periods Appointment Actions For each appointment, you can:\nConfirm Appointments:\nChange status from Pending to Confirmed Customer receives automatic email notification Complete Appointments:\nMark appointments as Completed after the meeting Cancel Appointments:\nCancel appointments with a reason System sends cancellation email to customer You cannot directly modify appointment times or dates. Contact your administrator if changes are needed.\nMy Schedule Page The My Schedule page displays your weekly availability.\nWeekly View See your time slots organized by day of week Color-coded slot status: Green: Available Gray: Booked/Unavailable View-Only Schedule Consultants have read-only access to their schedules. All schedule modifications must be made by administrators through the Admin Dashboard.\nTroubleshooting Issue: Cannot log in\nVerify you\u0026rsquo;re using the correct Consultant Portal URL (not Admin Dashboard) Reset password if needed You\u0026rsquo;re now ready to effectively manage your consulting appointments through the Consultant Portal!\n"},{"uri":"https://whopqa.github.io/AWS_Intern/5-workshop/5.12-cleanup/","title":"Clean Up Resources","tags":[],"description":"","content":"Clean Up Resources To avoid incurring unnecessary AWS charges, follow these steps to completely remove all resources created during this workshop.\nThis cleanup process is irreversible. All data, including customer information, appointments, and consultant records will be permanently deleted. Make sure to export any data you need before proceeding.\nStep 1: Destroy CDK Stack Navigate to your project directory and destroy the CDK stack:\ncd MeetAssist cdk destroy --all When prompted, confirm the deletion by typing y.\nThis will remove:\nLambda functions API Gateway RDS PostgreSQL database DynamoDB tables Cognito User Pools S3 buckets (except those with versioning/retention) CloudFront distributions SQS queues EventBridge rules IAM roles and policies The CDK destroy process may take 30-40 minutes to complete. Wait for confirmation before proceeding to the next step.\nStep 2: Delete S3 Buckets Some S3 buckets may not be automatically deleted if they contain objects. Manually delete them:\nList your buckets:\naws s3 ls | grep meetassist Empty and delete each bucket:\naws s3 rm s3://meetassist-data-\u0026lt;account-id\u0026gt;-ap-northeast-1 --recursive aws s3 rb s3://meetassist-data-\u0026lt;account-id\u0026gt;-ap-northeast-1 aws s3 rm s3://meetassist-dashboard-\u0026lt;account-id\u0026gt;-ap-northeast-1 --recursive aws s3 rb s3://meetassist-dashboard-\u0026lt;account-id\u0026gt;-ap-northeast-1 Replace \u0026lt;account-id\u0026gt; with your actual AWS account ID.\nStep 3: Remove Cognito Users If you manually created any Cognito users that weren\u0026rsquo;t deleted:\naws cognito-idp list-users --user-pool-id \u0026lt;your-user-pool-id\u0026gt; --region ap-northeast-1 The CDK destroy should have removed the User Pool, but verify in the console that no orphaned resources remain.\nStep 4: Delete Secrets Manager Secrets Check for any remaining secrets:\naws secretsmanager list-secrets --region ap-northeast-1 | grep meetassist Delete any found secrets:\naws secretsmanager delete-secret --secret-id MeetAssist/Facebook/PageAccessToken --region ap-northeast-1 --force-delete-without-recovery aws secretsmanager delete-secret --secret-id MeetAssist/Facebook/VerifyToken --region ap-northeast-1 --force-delete-without-recovery Step 5: Delete SSM Parameters Remove the Facebook App ID and App Secret:\naws ssm delete-parameter --name /MeetAssist/Facebook/AppId --region ap-northeast-1 aws ssm delete-parameter --name /MeetAssist/Facebook/AppSecret --region ap-northeast-1 Step 6: Remove Facebook App Configuration Go to Facebook Developers Navigate to your MeetAssist app Go to Settings → Basic Scroll down and click Delete App Confirm the deletion Alternatively, you can keep the Facebook App but remove the webhook subscription and page connection if you plan to rebuild the project later.\nStep 7: Disable Bedrock Models (Optional) If you no longer need access to the Bedrock models:\nGo to AWS Console → Amazon Bedrock Navigate to Model access in the left sidebar For each enabled model: Claude 3.5 Sonnet Claude 3 Sonnet Claude 3 Haiku Titan Embeddings G1 - Text Click Manage → Disable access Bedrock model access itself doesn\u0026rsquo;t incur charges. You\u0026rsquo;re only billed for API calls. You can leave the models enabled if you plan to use them in other projects.\nStep 8: Verify Cleanup Double-check that all resources are removed:\nCheck CloudFormation Stacks:\naws cloudformation list-stacks --region ap-northeast-1 | grep MeetAssist Check Lambda Functions:\naws lambda list-functions --region ap-northeast-1 | grep MeetAssist Check RDS Instances:\naws rds describe-db-instances --region ap-northeast-1 | grep meetassist Check DynamoDB Tables:\naws dynamodb list-tables --region ap-northeast-1 | grep MeetAssist All commands should return empty results.\nCost Considerations After cleanup, you should see the following in your AWS billing:\nOngoing Charges (None):\nLambda, API Gateway, RDS, DynamoDB - $0 (resources deleted) CloudFront - $0 (distribution deleted) S3 storage - $0 (buckets emptied and deleted) One-time Charges:\nBedrock API calls - Based on usage during workshop Data transfer costs - Minimal SES emails sent - Negligible cost Monitor your AWS Cost Explorer for 2-3 days after cleanup to ensure no unexpected charges appear.\nTroubleshooting Cleanup Issues Issue: CDK destroy fails\nCheck CloudFormation console for specific error messages Manually delete stuck resources through AWS Console Retry cdk destroy after manual intervention Issue: RDS instance not deleting\nCheck if deletion protection is enabled Disable in RDS Console → Modify → uncheck \u0026ldquo;Enable deletion protection\u0026rdquo; Create final snapshot or skip if not needed Issue: Still seeing charges\nCheck for resources in other regions (this workshop uses ap-northeast-1) Look for CloudWatch Logs log groups (can accumulate storage costs) Review Cost Explorer for detailed breakdown Congratulations! You\u0026rsquo;ve successfully cleaned up all workshop resources. Thank you for completing this workshop!\n"},{"uri":"https://whopqa.github.io/AWS_Intern/1-worklog/1.13-week13/","title":"Week 13 Worklog","tags":[],"description":"","content":"Week 13 Objectives: Finalize and optimize appointment scheduling flow with edge case handling. Address throttling and timeout issues in the system. Improve prompt engineering to increase bot accuracy. Optimize vector search and schema retrieval. Deploy, test, and fix bugs in production environment. Complete documentation and demo. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Continue developing appointment scheduling flow alongside deploy testing - Add Webhook Receiver to handle requests from Messenger: + Deploy SQS to avoid Lambda timeout + Prevent multiple response returns - Handle throttling error when invoking Lambda (too many bot calls) - Improve prompt for SQL mutation: + Fix column, table, param errors + Add annotations for enum params and user input params - Optimize schema search using embedding: + Create filter function to get top closest tables + Improve similarity score by adding string annotations to table names - Update permissions: + Admin for mutation SQL (DB admin secret) + Readonly for SELECT SQL 12/01/2025 12/01/2025 https://cloudjourney.awsstudygroup.com/ 2 - Merge 2 development branches and deploy test - Handle discovered issues: + Mutation SQL prompt lacks flexibility when schema changes + Hallucination: Bot responds normally when no SQL results + Throttling still occurs + Response too long (\u0026gt;2000 characters) prevents Messenger processing - Improve prompts: + Add prompt to respond based on SQL results (no results → respond according to context) + Adjust extraction prompt to understand final intent before extracting information + Avoid bias into context + Require output \u0026lt;2000 characters - Test model upgrade: + Switch Claude 3 Haiku → Claude 3.5 Sonnet + Use cross-region (trade-off higher latency → timeout) 12/03/2025 12/03/2025 https://cloudjourney.awsstudygroup.com/ 2 - Update new appointment scheduling flow: Create Flow: • Information extraction prompt starts when intent is booking create • Switch to collecting state • No need to call intent classification prompt in collecting state • Call extraction prompt → check field → response ask → allow user to ask more • Repeat until consultant name, date, time are sufficient • Display waiting message → get available slots → cache index • User selects slot → confirm → confirm state → mutation → notify → idle Update Flow: • Intent update → selecting slots state • Get booked appointments by customer ID → cache index → user selects • Save old and new appointment params → switch to collecting state • Update: Cancel old appointment + Insert new appointment Delete Flow: • Get appointments by customer ID → user selects → confirm → mutation cancel 12/03/2025 12/03/2025 https://cloudjourney.awsstudygroup.com/ 4 - Adjust model temperature to increase accuracy - Fix Vietnamese comparison error: + Don\u0026rsquo;t use regular ILIKE + Use unaccent in DB schema - Improve ID retrieval prompt: + More flexible with soft information (consultant name) + Use SELECT OR instead of requiring full name, date, time - Merge intent recognition and extraction info into one prompt (Claude 3 Sonnet) - Add constraints: + Query customer information only by ID + Get consultant schedule only for current and future time 12/05/2025 12/05/2025 https://cloudjourney.awsstudygroup.com/ 6 - Complete smarter welcome message - Modifications: + Only get user appointments with \u0026ldquo;pending\u0026rdquo; status + Constraint for getting appointments: time \u0026gt; current_time if day = current_day - Revise proposal document - Write README documentation - Prepare and perform system demo 12/07/2025 12/07/2025 https://cloudjourney.awsstudygroup.com/ Week 13 Achievements: Handling Throttling and Timeout Issues:\nDeployed SQS as message queue between Messenger and Lambda: Webhook receiver receives requests from Messenger and pushes to SQS Lambda consumer processes messages from queue Prevents multiple response returns due to timeout Minimizes throttling errors when too many concurrent requests Handled Lambda invocation throttling error during SQL mutation for appointments Configured database access permissions: Admin secret for mutation SQL (INSERT/UPDATE/DELETE) Readonly permission for SELECT SQL Optimizing Vector Search and Schema Retrieval:\nImproved schema search efficiency using embedding: Created filter function to get top tables with highest similarity Added string annotations for each table name to improve similarity score Reduced number of returned schemas from too many to only most relevant tables Addressed low similarity score and inaccurate results issues Improving Prompt Engineering:\nUpgraded prompt for SQL mutation: Fixed column, table, parameter errors Added detailed guidance on enum parameters Clarified which parameters users need to input Increased flexibility when schema changes Addressed hallucination issues: Bot responds based on actual SQL results When no SQL results → respond according to context instead of rigid responses Avoid bias into irrelevant context Optimized output: Required response \u0026lt;2000 characters for Messenger processing Adjusted model temperature to increase accuracy Improved extraction prompt: Understand user\u0026rsquo;s final intent before extracting information to fill fields Merged intent recognition and extraction info into one prompt for Claude 3 Sonnet Completing Appointment Scheduling Flow:\nCreate Flow (New appointment):\nIdentify intent as booking create → switch to collecting state No need to call intent classification prompt in collecting state Call information extraction prompt → check required fields Return response asking for missing information → allow user to ask more Repeat until consultant name, date, time are sufficient Display waiting message → call prompt to get available slots Cache index new appointment information (consultant id, name, date, time) User selects slot → get information as params User confirms → confirm state → mutation prompt → execute Notify success/failure → return to idle state Update Flow (Update appointment):\nIdentify intent update → selecting slots state Automatically get booked appointments by customer ID → cache index User selects appointment to change Save time, date, consultant id of old appointment to params Save customer id, email, phone to new appointment params Switch to collecting state (similar to create) Execute: Cancel old appointment (update status) + Insert new appointment Delete Flow (Cancel appointment):\nIdentify intent delete Automatically get booked appointments by customer ID → cache index User selects appointment to cancel Get old appointment information User confirms → mutation SQL (update status to \u0026ldquo;cancelled\u0026rdquo;) Handling Vietnamese and Database:\nFixed Vietnamese comparison error: Don\u0026rsquo;t use regular ILIKE Configure and use unaccent extension in PostgreSQL Re-setup in DB schema to support Vietnamese search without diacritics Improved ID retrieval prompt based on soft information: More flexible with information like consultant name Use SELECT OR instead of requiring full consultant name, date, and time Model Testing and Cross-Region:\nUpgraded from Claude 3 Haiku to Claude 3.5 Sonnet: Improved response quality Better context understanding Used cross-region inference: Trade-off between model quality and latency Higher latency but may cause timeout Need to balance between performance and latency Constraints and Business Logic:\nAdded business constraints: Query customer information only by customer ID Get consultant schedule only for current and future time Only get user appointments with \u0026ldquo;pending\u0026rdquo; status Get appointments: time \u0026gt; current_time if day = current_day Completed smarter welcome message with context Documentation and Demo:\nRevised and completed proposal document Wrote comprehensive README documentation Successfully prepared and performed system demo Lessons Learned and Insights:\nModel temperature significantly affects information extraction accuracy Merging multiple prompts into one can reduce API calls and increase consistency Vector search needs careful fine-tuning with annotations to achieve good similarity Cross-region inference is a trade-off between model quality and latency SQS is an effective solution for throttling and timeout issues Vietnamese handling requires unaccent extension in PostgreSQL Business logic constraints are crucial to avoid invalid data "},{"uri":"https://whopqa.github.io/AWS_Intern/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://whopqa.github.io/AWS_Intern/tags/","title":"Tags","tags":[],"description":"","content":""}]